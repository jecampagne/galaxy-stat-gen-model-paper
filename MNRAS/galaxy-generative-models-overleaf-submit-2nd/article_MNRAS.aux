\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ramesh2022}
\citation{Oppenlaender2022}
\citation{Rombach2022}
\citation{Kingma2014}
\citation{goodfellow2014generative}
\citation{KarrasALL18,brock2018large}
\citation{Kingma2018}
\citation{PhysRevD.107.076017}
\citation{ravanbakhsh2016}
\citation{Schawinski2017,Fussell2019,Hemmati_2022}
\citation{Arcelin2020}
\citation{Lanusse2021}
\citation{smith2021}
\citation{ROWE2015121}
\citation{Hataya2023}
\citation{mandelbaum_2019_3242143}
\citation{sdss}
\citation{sdssdr7}
\newlabel{firstpage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:Intro}{{1}{1}{Introduction}{section.1}{}}
\citation{HACKSTEIN2023100685}
\citation{janulewicz2024assessing}
\citation{kadkhodaie2024generalization}
\citation{kadkhodaie2024generalization}
\citation{ronneberger2015u}
\citation{Mohan2020Robust}
\citation{Liu2015}
\citation{Yu2015}
\citation{Kingma2014}
\citation{Kingma2014}
\citation{Kingma2014}
\citation{engel2018latent,Takida2022}
\citation{Lanusse2021}
\@writefile{toc}{\contentsline {section}{\numberline {2}Generative Models}{2}{section.2}\protected@file@percent }
\newlabel{sec-generative-models}{{2}{2}{Generative Models}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Variational Auto-Encoder}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq-VAE-ELBO}{{4}{2}{Variational Auto-Encoder}{equation.4}{}}
\citation{goodfellow2014generative}
\citation{Brock2019}
\citation{Gulrajani2017}
\citation{Saxena2021,Jozdani2022}
\citation{Brock2019}
\citation{Karras2018ASG,Karras2020}
\citation{liu2021towards}
\citation{Lim2017}
\citation{Vapnik1997}
\citation{Schawinski2017}
\citation{Hemmati_2022}
\citation{Coccomini2021}
\citation{Tabak2010,Tabak2013a,Rezende2015}
\citation{Bogachev2005,huang2019solving}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Generative Adversarial Network}{3}{subsection.2.2}\protected@file@percent }
\newlabel{sec-GAN}{{2.2}{3}{Generative Adversarial Network}{subsection.2.2}{}}
\newlabel{eq-discri-hinge-loss}{{8}{3}{Generative Adversarial Network}{equation.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic Normalizing Flow process. On the top the \textit  {forward} or \textit  {generative} direction from a simple $\bm  {z}$ distribution to a more complex one for $\bm  {x}$. On the bottom the \textit  {backward} or \textit  {training} direction from complex to simple distributions.\relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-normflow}{{1}{3}{Schematic Normalizing Flow process. On the top the \textit {forward} or \textit {generative} direction from a simple $\bm {z}$ distribution to a more complex one for $\bm {x}$. On the bottom the \textit {backward} or \textit {training} direction from complex to simple distributions.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Normalizing Flows}{3}{subsection.2.3}\protected@file@percent }
\newlabel{sec-NF}{{2.3}{3}{Normalizing Flows}{subsection.2.3}{}}
\newlabel{eq-flow-jacob}{{11}{3}{Normalizing Flows}{equation.11}{}}
\citation{Papamakarios2021}
\citation{Vaswani2017}
\citation{DinhKB14,Papamakarios2017a,DinhSB17,Kingma2018}
\citation{Crenshaw_2024}
\citation{DinhKB14,DinhSB17}
\citation{DinhSB17}
\citation{Kingma2018}
\citation{DinhSB17}
\citation{DinhSB17}
\citation{Kingma2018}
\citation{DinhSB17}
\citation{Kingma2018}
\citation{DinhSB17}
\citation{Chang2023,LinYang2023}
\newlabel{eq-NF-loss-1}{{12}{4}{Normalizing Flows}{equation.12}{}}
\newlabel{eq-NF-loss-2}{{13}{4}{Normalizing Flows}{equation.13}{}}
\newlabel{eq-affine-coupling}{{15}{4}{Normalizing Flows}{equation.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Copy of Figure 2 of \citep  {Kingma2018}. The flow step consists of a scale and bias layer with data-dependent initialization (\texttt  {actnorm}), a $1\times 1$ invertible convolution layer, and an affine coupling layer where $(\bm  {\alpha },\bm  {\beta })$ are derived from a shallow convolutional neural network. Both $\bm  {x}$ and $\bm  {y}$ are tensors of shape $[h \times w \times c]$, where $(h, w)$ are the spatial dimensions and $c$ is the channel dimension. The indices $(i, j)$ refer to spatial positions in tensors $\bm  {x}$ and $\bm  {y}$. This flow step is embedded within a multi-scale architecture consisting of $K$ flow steps and $L$ levels \citep  {DinhSB17}. The squeezing operation transforms a tensor of shape $s \times s \times c$ into one of shape $s/2 \times s/2 \times 4c$, mixing spatial and channel components. At each scale, the channel image is divided into $2\times 2$ patches ($s=2$), and each patch undergoes the squeezing operation.\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig-Glow-archi}{{2}{4}{Copy of Figure 2 of \citep {Kingma2018}. The flow step consists of a scale and bias layer with data-dependent initialization (\texttt {actnorm}), a $1\times 1$ invertible convolution layer, and an affine coupling layer where $(\bm {\alpha },\bm {\beta })$ are derived from a shallow convolutional neural network. Both $\bm {x}$ and $\bm {y}$ are tensors of shape $[h \times w \times c]$, where $(h, w)$ are the spatial dimensions and $c$ is the channel dimension. The indices $(i, j)$ refer to spatial positions in tensors $\bm {x}$ and $\bm {y}$. This flow step is embedded within a multi-scale architecture consisting of $K$ flow steps and $L$ levels \citep {DinhSB17}. The squeezing operation transforms a tensor of shape $s \times s \times c$ into one of shape $s/2 \times s/2 \times 4c$, mixing spatial and channel components. At each scale, the channel image is divided into $2\times 2$ patches ($s=2$), and each patch undergoes the squeezing operation.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Score-based diffusion models}{4}{subsection.2.4}\protected@file@percent }
\newlabel{sec-diff}{{2.4}{4}{Score-based diffusion models}{subsection.2.4}{}}
\citation{guth2022wavelet}
\citation{Uhlenbeck1930}
\citation{hyvarinen2005a}
\citation{Bengio2013}
\citation{Sohl-Dickstein2015,Ho2020,song2020score}
\citation{kadkhodaie2024generalization}
\citation{Guth2022b,Lempereur2024}
\citation{smith2021}
\citation{Zhao2023}
\citation{Schanz2023}
\citation{Remy2023}
\citation{tweedie1947functions,herbert1956empirical,miyasawa1961empirical}
\citation{Raphan2011}
\citation{dhariwal2021diffusion}
\citation{zhang2021diffusion,gong2021interpreting}
\citation{Wang2023}
\newlabel{eq-Ornstein-cont}{{16}{5}{Score-based diffusion models}{equation.16}{}}
\newlabel{eq-smoothing-p0-gauss}{{18}{5}{Score-based diffusion models}{equation.18}{}}
\newlabel{eq-backward-diffusion}{{20}{5}{Score-based diffusion models}{equation.20}{}}
\newlabel{thm-score-denoising}{{21}{5}{Score-based diffusion models}{equation.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Some comments}{5}{subsection.2.5}\protected@file@percent }
\newlabel{sec-some-comments}{{2.5}{5}{Some comments}{subsection.2.5}{}}
\citation{Brock2019}
\citation{kadkhodaie2024generalization}
\citation{PyTorch2019}
\citation{HACKSTEIN2023100685}
\citation{HACKSTEIN2023100685}
\citation{corner2016}
\citation{Kingma2018}
\citation{ronneberger2015u}
\citation{kadkhodaie2024generalization}
\citation{kadkhodaie2024generalization}
\citation{smith2021}
\citation{2020SciPy-NMeth}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiment}{6}{section.3}\protected@file@percent }
\newlabel{sec-experiment}{{3}{6}{Experiment}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The models}{6}{subsection.3.1}\protected@file@percent }
\newlabel{sec-Exp-Models}{{3.1}{6}{The models}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The dataset}{6}{subsection.3.2}\protected@file@percent }
\newlabel{sec-Dataset}{{3.2}{6}{The dataset}{subsection.3.2}{}}
\citation{2019MNRAS.483.4140R}
\citation{2000AJ....119.2645B,2003ApJS..147....1C,2004AJ....128..163L}
\citation{2004AJ....128..163L,10.1093/mnras/stv2078}
\citation{ravanbakhsh2016,Fussell2019,Lanusse2021,smith2021,HACKSTEIN2023100685}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Examples of \textit  {validation} images from the original dataset (top row) and generated samples from different models trained with $N = 10^5$ images: the \texttt  {light-weight-gan} GAN-based model (second row), the \texttt  {Glow} flow-based model (third row), and the diffusion-based model with a \texttt  {U-Net} denoiser (bottom row).\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig-Original-Glow-UNet-Gan-samples}{{3}{7}{Examples of \textit {validation} images from the original dataset (top row) and generated samples from different models trained with $N = 10^5$ images: the \texttt {light-weight-gan} GAN-based model (second row), the \texttt {Glow} flow-based model (third row), and the diffusion-based model with a \texttt {U-Net} denoiser (bottom row).\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Corner plots\footnotemark of morphological coefficients computed using either the validation dataset unused by any model trainings (\textit  {red, "test"}) or generated images from the diffusion-based model (\textit  {blue, "diffusion"}), the \texttt  {Glow} flow-based model (\textit  {green, "glow"}), and the \texttt  {light-weight-gan} GAN-based model (\textit  {cyan, "gan"}). All models were trained with $N = 10^5$ galaxy images and correspond to the results shown in Figure\nobreakspace  {}\ref  {fig-Original-Glow-UNet-Gan-samples}. The plot design is inspired by \cite {HACKSTEIN2023100685}.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig-morpho-coeff}{{4}{7}{Corner plots\protect \footnotemark of morphological coefficients computed using either the validation dataset unused by any model trainings (\textit {red, "test"}) or generated images from the diffusion-based model (\textit {blue, "diffusion"}), the \texttt {Glow} flow-based model (\textit {green, "glow"}), and the \texttt {light-weight-gan} GAN-based model (\textit {cyan, "gan"}). All models were trained with $N = 10^5$ galaxy images and correspond to the results shown in Figure~\ref {fig-Original-Glow-UNet-Gan-samples}. The plot design is inspired by \protect \cite {HACKSTEIN2023100685}.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results}{7}{subsection.3.3}\protected@file@percent }
\newlabel{sec-Results}{{3.3}{7}{Results}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Generated images}{7}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{sec-Generated-Images}{{3.3.1}{7}{Generated images}{subsubsection.3.3.1}{}}
\citation{books/aw/TanSK2005}
\citation{kadkhodaie2024generalization}
\citation{kadkhodaie2024generalization}
\citation{kadkhodaie2024generalization}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Denoising performances of two \texttt  {U-Net} models on an image from the \textit  {training sample}. Top row: (left to right) the original image with progressively increasing noise added. The \texttt  {PSNR} is defined as $10\qopname  \relax o{log}_{10}$ ratio of the squared dynamic range to the mean square error. Middle row: (left to right) denoised images and their PSNR values obtained using a \texttt  {U-Net} trained with a dataset of size $N=100$. Bottom row: same as the middle row but with a \texttt  {U-Net} trained with a much larger dataset of $N=100,000$ images.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig-UNet-denoising-train}{{5}{8}{Denoising performances of two \texttt {U-Net} models on an image from the \textit {training sample}. Top row: (left to right) the original image with progressively increasing noise added. The \texttt {PSNR} is defined as $10\log _{10}$ ratio of the squared dynamic range to the mean square error. Middle row: (left to right) denoised images and their PSNR values obtained using a \texttt {U-Net} trained with a dataset of size $N=100$. Bottom row: same as the middle row but with a \texttt {U-Net} trained with a much larger dataset of $N=100,000$ images.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Similar to Figure\nobreakspace  {}\ref  {fig-UNet-denoising-train}, but applied to an image that was never used to train the networks (a \textit  {validation} image).\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig-UNet-denoising-test}{{6}{8}{Similar to Figure~\ref {fig-UNet-denoising-train}, but applied to an image that was never used to train the networks (a \textit {validation} image).\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}\texttt  {U-Net} Denoising Performances}{8}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{sec-UNet-Perf}{{3.3.2}{8}{\texttt {U-Net} Denoising Performances}{subsubsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Two Diffusion-Based Models Test}{8}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{sec-two_models-Diffusion}{{3.3.3}{8}{Two Diffusion-Based Models Test}{subsubsection.3.3.3}{}}
\citation{kadkhodaie2024generalization}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Evolution of the cosine similarity metric for diffusion-based models. Top row: normalized histograms of cosine similarity values between $1,000$ samples generated by two models trained on independent datasets of size $N$, using the same white noise images as seeds. Bottom row: maximum cosine similarity between $1,000$ generated samples from a single model and all images in its corresponding training dataset (i.e., the cosine of the closest training image for each generated sample). The logarithmic vertical scale and linear horizontal scale (cosine value) are consistent across all histograms on each row. The horizontal range in the bottom row is restricted to values near the maximum (cosine = 1), indicated by the dashed red vertical line. This figure is inspired by Figure 2 of \cite {kadkhodaie2024generalization}.\relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig-cosine-diffusion}{{7}{9}{Evolution of the cosine similarity metric for diffusion-based models. Top row: normalized histograms of cosine similarity values between $1,000$ samples generated by two models trained on independent datasets of size $N$, using the same white noise images as seeds. Bottom row: maximum cosine similarity between $1,000$ generated samples from a single model and all images in its corresponding training dataset (i.e., the cosine of the closest training image for each generated sample). The logarithmic vertical scale and linear horizontal scale (cosine value) are consistent across all histograms on each row. The horizontal range in the bottom row is restricted to values near the maximum (cosine = 1), indicated by the dashed red vertical line. This figure is inspired by Figure 2 of \protect \cite {kadkhodaie2024generalization}.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Some morphological coefficients (see Figure\nobreakspace  {}\ref  {fig-morpho-coeff}) computed using diffusion models trained with different dataset sizes ($N=10^5, 10^4, 10^3$).\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig-morpho-coeff-diff-comparison}{{8}{9}{Some morphological coefficients (see Figure~\ref {fig-morpho-coeff}) computed using diffusion models trained with different dataset sizes ($N=10^5, 10^4, 10^3$).\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Histograms similar to those of Figure\nobreakspace  {}\ref  {fig-cosine-diffusion} for two \texttt  {Glow} models trained on independent datasets of size $N=100,000$ (identical to the datasets used by the diffusion models). Left: cosine similarity of $1,000$ samples generated by the two models seeded with the same white noise image. Right: maximum cosine similarity values of $1,000$ samples generated by one model compared to all training images and their augmented versions (horizontal, vertical, and horizontal+vertical flips), as these augmentations are part of the \texttt  {Glow} training process.\relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig-cosine-glow}{{9}{9}{Histograms similar to those of Figure~\ref {fig-cosine-diffusion} for two \texttt {Glow} models trained on independent datasets of size $N=100,000$ (identical to the datasets used by the diffusion models). Left: cosine similarity of $1,000$ samples generated by the two models seeded with the same white noise image. Right: maximum cosine similarity values of $1,000$ samples generated by one model compared to all training images and their augmented versions (horizontal, vertical, and horizontal+vertical flips), as these augmentations are part of the \texttt {Glow} training process.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Flow-Based Models Tests}{9}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{sec-two_models-Flow}{{3.3.4}{9}{Flow-Based Models Tests}{subsubsection.3.3.4}{}}
\citation{Lim2017}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Results of the "inversion test" using $1,000$ samples $\bm  {z}$ (see text). Top panel: Comparison of $z^\prime _{\texttt  {model}}$ vs. $z$ at all levels of the \texttt  {Glow} architecture (see Figure\nobreakspace  {}\ref  {fig-Glow-archi}) for Model A (blue dots) and Model B (orange dots). Bottom panel: Distributions of $z^\prime _{\texttt  {model}}[\ell ]$ for different levels $\ell = \{0, 1, 2 = L\}$, where solid (resp. dashed) curves correspond to Model A (resp. Model B). Both models were trained on independent datasets of $10^5$ galaxy images.\relax }}{10}{figure.caption.10}\protected@file@percent }
\newlabel{fig-glow-inversibility}{{10}{10}{Results of the "inversion test" using $1,000$ samples $\bm {z}$ (see text). Top panel: Comparison of $z^\prime _{\texttt {model}}$ vs. $z$ at all levels of the \texttt {Glow} architecture (see Figure~\ref {fig-Glow-archi}) for Model A (blue dots) and Model B (orange dots). Bottom panel: Distributions of $z^\prime _{\texttt {model}}[\ell ]$ for different levels $\ell = \{0, 1, 2 = L\}$, where solid (resp. dashed) curves correspond to Model A (resp. Model B). Both models were trained on independent datasets of $10^5$ galaxy images.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Same legend as bottom panel of Figure \ref  {fig-glow-inversibility} but the models have been trained with independent datasets composed of $10^3$ galaxy images. One notices that de $z_B^\prime $ distributions significantly tend to deviate from the $z$ (i.e. $\mathcal  {N}(\bm  {0},\bm  {1})$) contrary to the $z_A^\prime $ distributions.\relax }}{10}{figure.caption.12}\protected@file@percent }
\newlabel{fig-glow-inversibility-1000}{{11}{10}{Same legend as bottom panel of Figure \ref {fig-glow-inversibility} but the models have been trained with independent datasets composed of $10^3$ galaxy images. One notices that de $z_B^\prime $ distributions significantly tend to deviate from the $z$ (i.e. $\mathcal {N}(\bm {0},\bm {1})$) contrary to the $z_A^\prime $ distributions.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}GAN-based models tests}{10}{subsubsection.3.3.5}\protected@file@percent }
\newlabel{sec-two_models-GAN}{{3.3.5}{10}{GAN-based models tests}{subsubsection.3.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Values of the 1-Wasserstein metric ($W_1(X,Y)$) computed for \texttt  {Glow} models trained in the "two-models test" context with three different training dataset sizes. For $K=32$ flows and $L=3$ levels, and considering $1,000$ generated images, one obtains $6\times 10^6$ (resp. $3\times 10^6$) $z$ and $z^\prime $ samples at level 0 (resp. levels 1, 2). Using i.i.d. samples $(X,Y)$ from $\mathcal  {N}(0,1)$ yields $W_1(X,Y) \approx 7\times 10^{-4}$ (resp. $\approx 10^{-3}$) for level 0 (resp. levels 1, 2). The very low value of $W_1(z^\prime _A, z) = O(10^{-6})$ is a direct consequence of the $z^\prime _A = z$ property shown in the left panel of Figure\nobreakspace  {}\ref  {fig-glow-inversibility}.\relax }}{11}{table.caption.11}\protected@file@percent }
\newlabel{tab:Wasserstein-1-Glow-AB}{{1}{11}{Values of the 1-Wasserstein metric ($W_1(X,Y)$) computed for \texttt {Glow} models trained in the "two-models test" context with three different training dataset sizes. For $K=32$ flows and $L=3$ levels, and considering $1,000$ generated images, one obtains $6\times 10^6$ (resp. $3\times 10^6$) $z$ and $z^\prime $ samples at level 0 (resp. levels 1, 2). Using i.i.d. samples $(X,Y)$ from $\mathcal {N}(0,1)$ yields $W_1(X,Y) \approx 7\times 10^{-4}$ (resp. $\approx 10^{-3}$) for level 0 (resp. levels 1, 2). The very low value of $W_1(z^\prime _A, z) = O(10^{-6})$ is a direct consequence of the $z^\prime _A = z$ property shown in the left panel of Figure~\ref {fig-glow-inversibility}.\relax }{table.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Some morphological coefficients (see Figure\nobreakspace  {}\ref  {fig-morpho-coeff}) computed using \texttt  {Glow} models trained with different dataset sizes ($N=10^5, 10^4, 10^3$). We can notice a degradation of the $M_{20}$ distribution when the number of training images decreases.\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig-morpho-coeff-glow-comparison}{{12}{11}{Some morphological coefficients (see Figure~\ref {fig-morpho-coeff}) computed using \texttt {Glow} models trained with different dataset sizes ($N=10^5, 10^4, 10^3$). We can notice a degradation of the $M_{20}$ distribution when the number of training images decreases.\relax }{figure.caption.13}{}}
\newlabel{eq-hinge-loss-ab}{{22}{11}{GAN-based models tests}{equation.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Histograms similar to those of Figures \ref  {fig-cosine-diffusion}, \ref  {fig-cosine-glow} for two \texttt  {light-weight} GAN models using two different datasets of size $N=100,000$ (nb. they are the same as the ones used by the diffusion models). Left: the cosine similarity of $1,000$ samples generated by the two models seeded by the same white noise image. Right: the maximum cosine similarity values of $1,000$ samples generated by one of the models and the training images and their flipped versions (horizontal, vertical and horizontal+vertical) as this data augmentation is used in the training process.\relax }}{11}{figure.caption.14}\protected@file@percent }
\newlabel{fig-cosine-gan}{{13}{11}{Histograms similar to those of Figures \ref {fig-cosine-diffusion}, \ref {fig-cosine-glow} for two \texttt {light-weight} GAN models using two different datasets of size $N=100,000$ (nb. they are the same as the ones used by the diffusion models). Left: the cosine similarity of $1,000$ samples generated by the two models seeded by the same white noise image. Right: the maximum cosine similarity values of $1,000$ samples generated by one of the models and the training images and their flipped versions (horizontal, vertical and horizontal+vertical) as this data augmentation is used in the training process.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Evolution of the hinge loss (Equation\nobreakspace  {}\ref  {eq-hinge-loss-ab}) for the "discriminator test" across various training epochs for models "A" and "B". Here, \(D_M(\bm  {x}_{M^\prime })\) represents the output of the discriminator of model \(M\) applied to samples generated by the generator of model \(M^\prime \), while \(\bm  {x}_{\text  {train}}\) refers to samples from the respective training dataset used for both models. All models were trained using datasets consisting of $N=10^5$ images.\relax }}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig-gan-hinge-loss-evol}{{14}{12}{Evolution of the hinge loss (Equation~\ref {eq-hinge-loss-ab}) for the "discriminator test" across various training epochs for models "A" and "B". Here, \(D_M(\bm {x}_{M^\prime })\) represents the output of the discriminator of model \(M\) applied to samples generated by the generator of model \(M^\prime \), while \(\bm {x}_{\text {train}}\) refers to samples from the respective training dataset used for both models. All models were trained using datasets consisting of $N=10^5$ images.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Some morphological coefficients (see Figure\nobreakspace  {}\ref  {fig-morpho-coeff}) computed using \texttt  {light-weight-gan} models trained with different dataset sizes ($N=10^5, 10^4, 10^3$).\relax }}{12}{figure.caption.16}\protected@file@percent }
\newlabel{fig-morpho-coeff-gan-comparison}{{15}{12}{Some morphological coefficients (see Figure~\ref {fig-morpho-coeff}) computed using \texttt {light-weight-gan} models trained with different dataset sizes ($N=10^5, 10^4, 10^3$).\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{12}{section.4}\protected@file@percent }
\newlabel{sec-conclusion}{{4}{12}{Conclusion}{section.4}{}}
\citation{kadkhodaie2024generalization}
\citation{kadkhodaie2024generalization}
\citation{Kingma2018}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Same legend as for Figure\nobreakspace  {}\ref  {fig-gan-hinge-loss-evol} but with different dataset sizes: $N=10^4$ (a) and $N=10^3$ (b).\relax }}{13}{figure.caption.17}\protected@file@percent }
\newlabel{fig-gan-hinge-loss-epoch100-ntrain-10p3-10p4}{{16}{13}{Same legend as for Figure~\ref {fig-gan-hinge-loss-evol} but with different dataset sizes: $N=10^4$ (a) and $N=10^3$ (b).\relax }{figure.caption.17}{}}
\bibstyle{mnras}
\bibdata{references}
\bibcite{sdssdr7}{{1}{2009}{{Abazajian et~al.}}{{Abazajian et~al.,}}}
\bibcite{Arcelin2020}{{2}{2020}{{Arcelin et~al.}}{{Arcelin, Doux, Aubourg, Roucelle \& Collaboration)}}}
\bibcite{Bengio2013}{{3}{2013}{{Bengio et~al.}}{{Bengio, Yao, Alain \& Vincent}}}
\bibcite{2000AJ....119.2645B}{{4}{2000}{{{Bershady} et~al.}}{{{Bershady}, {Jangren} \& {Conselice}}}}
\bibcite{Bogachev2005}{{5}{2005}{{{Bogachev} et~al.}}{{{Bogachev}, {Kolesnikov} \& {Medvedev}}}}
\bibcite{brock2018large}{{6}{2019a}{{Brock et~al.}}{{Brock, Donahue \& Simonyan}}}
\bibcite{Brock2019}{{7}{2019b}{{Brock et~al.}}{{Brock, Donahue \& Simonyan}}}
\bibcite{Chang2023}{{8}{2023}{{{Chang} et~al.}}{{{Chang}, {Koulieris} \& {Shum}}}}
\bibcite{Coccomini2021}{{9}{2021}{{{Coccomini} et~al.}}{{{Coccomini}, {Messina}, {Gennaro} \& {Falchi}}}}
\bibcite{2003ApJS..147....1C}{{10}{2003}{{{Conselice}}}{{{Conselice}}}}
\bibcite{Crenshaw_2024}{{11}{2024}{{Crenshaw et~al.}}{{Crenshaw, Kalmbach, Gagliano, Yan, Connolly, Malz, Schmidt \& Collaboration}}}
\bibcite{dhariwal2021diffusion}{{12}{2021}{{Dhariwal \& Nichol}}{{Dhariwal \& Nichol}}}
\bibcite{DinhKB14}{{13}{2015}{{Dinh et~al.}}{{Dinh, Krueger \& Bengio}}}
\bibcite{DinhSB17}{{14}{2017}{{Dinh et~al.}}{{Dinh, Sohl{-}Dickstein \& Bengio}}}
\bibcite{engel2018latent}{{15}{2018}{{Engel et~al.}}{{Engel, Hoffman \& Roberts}}}
\bibcite{corner2016}{{16}{2016}{{Foreman-Mackey}}{{Foreman-Mackey}}}
\bibcite{Fussell2019}{{17}{2019}{{Fussell \& Moews}}{{Fussell \& Moews}}}
\bibcite{gong2021interpreting}{{18}{2021}{{Gong \& Li}}{{Gong \& Li}}}
\bibcite{goodfellow2014generative}{{19}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville \& Bengio}}}
\bibcite{Gulrajani2017}{{20}{2017}{{Gulrajani et~al.}}{{Gulrajani, Ahmed, Arjovsky, Dumoulin \& Courville}}}
\bibcite{guth2022wavelet}{{21}{2022a}{{Guth et~al.}}{{Guth, Coste, De~Bortoli \& Mallat}}}
\bibcite{Guth2022b}{{22}{2022b}{{Guth et~al.}}{{Guth, Coste, De~Bortoli \& Mallat}}}
\bibcite{HACKSTEIN2023100685}{{23}{2023}{{Hackstein et~al.}}{{Hackstein, Kinakh, Bailer \& Melchior}}}
\bibcite{Hataya2023}{{24}{2023}{{Hataya et~al.}}{{Hataya, Bao \& Arai}}}
\bibcite{Hemmati_2022}{{25}{2022}{{Hemmati et~al.}}{{Hemmati et~al.,}}}
\bibcite{herbert1956empirical}{{26}{1956}{{Herbert}}{{Herbert}}}
\bibcite{Ho2020}{{27}{2020}{{Ho et~al.}}{{Ho, Jain \& Abbeel}}}
\bibcite{huang2019solving}{{28}{2019}{{Huang et~al.}}{{Huang, Dinh \& Courville}}}
\bibcite{hyvarinen2005a}{{29}{2005}{{Hyv{{\"a}}rinen}}{{Hyv{{\"a}}rinen}}}
\bibcite{janulewicz2024assessing}{{30}{2024}{{Janulewicz et~al.}}{{Janulewicz, Perreault-Levasseur \& Webb}}}
\bibcite{Jozdani2022}{{31}{2022}{{Jozdani et~al.}}{{Jozdani, Chen, Pouliot \& {Alan Johnson}}}}
\bibcite{kadkhodaie2024generalization}{{32}{2024}{{Kadkhodaie et~al.}}{{Kadkhodaie, Guth, Simoncelli \& Mallat}}}
\bibcite{PhysRevD.107.076017}{{33}{2023}{{Kansal et~al.}}{{Kansal, Li, Duarte, Chernyavskaya, Pierini, Orzari \& Tomei}}}
\bibcite{KarrasALL18}{{34}{2018a}{{Karras et~al.}}{{Karras, Aila, Laine \& Lehtinen}}}
\bibcite{Karras2018ASG}{{35}{2018b}{{Karras et~al.}}{{Karras, Laine \& Aila}}}
\bibcite{Karras2020}{{36}{2020}{{Karras et~al.}}{{Karras, Laine, Aittala, Hellsten, Lehtinen \& Aila}}}
\bibcite{Kingma2018}{{37}{2018}{{Kingma \& Dhariwal}}{{Kingma \& Dhariwal}}}
\bibcite{Kingma2014}{{38}{2014}{{Kingma \& Welling}}{{Kingma \& Welling}}}
\bibcite{Lanusse2021}{{39}{2021}{{Lanusse et~al.}}{{Lanusse, Mandelbaum, Ravanbakhsh, Li, Freeman \& PÃ³czos}}}
\bibcite{Lempereur2024}{{40}{2024}{{{Lempereur} \& {Mallat}}}{{{Lempereur} \& {Mallat}}}}
\bibcite{Lim2017}{{41}{2017}{{{Lim} \& {Ye}}}{{{Lim} \& {Ye}}}}
\bibcite{Liu2015}{{42}{2015}{{Liu et~al.}}{{Liu, Luo, Wang \& Tang}}}
\bibcite{liu2021towards}{{43}{2021}{{Liu et~al.}}{{Liu, Zhu, Song \& Elgammal}}}
\bibcite{2004AJ....128..163L}{{44}{2004}{{{Lotz} et~al.}}{{{Lotz}, {Primack} \& {Madau}}}}
\bibcite{mandelbaum_2019_3242143}{{45}{2019}{{Mandelbaum et~al.}}{{Mandelbaum, Lackner, Leauthaud \& Rowe}}}
\bibcite{miyasawa1961empirical}{{46}{1961}{{Miyasawa et~al.}}{{Miyasawa et~al.}}}
\bibcite{Mohan2020Robust}{{47}{2020}{{Mohan et~al.}}{{Mohan, Kadkhodaie, Simoncelli \& Fernandez-Granda}}}
\bibcite{Oppenlaender2022}{{48}{2022}{{Oppenlaender}}{{Oppenlaender}}}
\bibcite{Papamakarios2017a}{{49}{2017}{{Papamakarios et~al.}}{{Papamakarios, Pavlakou \& Murray}}}
\bibcite{Papamakarios2021}{{50}{2021}{{Papamakarios et~al.}}{{Papamakarios, Nalisnick, Rezende, Mohamed \& Lakshminarayanan}}}
\bibcite{PyTorch2019}{{51}{2019}{{{Paszke} et~al.}}{{{Paszke} et~al.,}}}
\bibcite{ramesh2022}{{52}{2022}{{{Ramesh} et~al.}}{{{Ramesh}, {Dhariwal}, {Nichol}, {Chu} \& {Chen}}}}
\bibcite{Raphan2011}{{53}{2011}{{Raphan \& Simoncelli}}{{Raphan \& Simoncelli}}}
\bibcite{ravanbakhsh2016}{{54}{2017}{{Ravanbakhsh et~al.}}{{Ravanbakhsh, Lanusse, Mandelbaum, Schneider \& Poczos}}}
\bibcite{Remy2023}{{55}{2023}{{{Remy, B.} et~al.}}{{{Remy, B.}, {Lanusse, F.}, {Jeffrey, N.}, {Liu, J.}, {Starck, J.-L.}, {Osato, K.} \& {Schrabback, T.}}}}
\bibcite{Rezende2015}{{56}{2015}{{Rezende \& Mohamed}}{{Rezende \& Mohamed}}}
\bibcite{2019MNRAS.483.4140R}{{57}{2019}{{{Rodriguez-Gomez} et~al.}}{{{Rodriguez-Gomez} et~al.,}}}
\bibcite{Rombach2022}{{58}{2022}{{Rombach et~al.}}{{Rombach, Blattmann, Lorenz, Esser \& Ommer}}}
\bibcite{ronneberger2015u}{{59}{2015}{{Ronneberger et~al.}}{{Ronneberger, Fischer \& Brox}}}
\bibcite{ROWE2015121}{{60}{2015}{{Rowe et~al.}}{{Rowe et~al.,}}}
\bibcite{Saxena2021}{{61}{2021}{{Saxena \& Cao}}{{Saxena \& Cao}}}
\bibcite{Schanz2023}{{62}{2023}{{{Schanz} et~al.}}{{{Schanz}, {List} \& {Hahn}}}}
\bibcite{Schawinski2017}{{63}{2017}{{Schawinski et~al.}}{{Schawinski, Zhang, Zhang, Fowler \& Santhanam}}}
\bibcite{smith2021}{{64}{2022}{{Smith et~al.}}{{Smith, Geach, Jackson, Arora, Stone \& Courteau}}}
\bibcite{10.1093/mnras/stv2078}{{65}{2015}{{Snyder et~al.}}{{Snyder et~al.,}}}
\bibcite{Sohl-Dickstein2015}{{66}{2015}{{Sohl-Dickstein et~al.}}{{Sohl-Dickstein, Weiss, Maheswaranathan \& Ganguli}}}
\bibcite{song2020score}{{67}{2020}{{Song et~al.}}{{Song, Sohl-Dickstein, Kingma, Kumar, Ermon \& Poole}}}
\bibcite{Tabak2013a}{{68}{2013}{{Tabak \& Turner}}{{Tabak \& Turner}}}
\bibcite{Tabak2010}{{69}{2010}{{Tabak \& Vanden-Eijnden}}{{Tabak \& Vanden-Eijnden}}}
\bibcite{Takida2022}{{70}{2022}{{Takida et~al.}}{{Takida, Liao, Lai, Uesaka, Takahashi \& Mitsufuji}}}
\bibcite{books/aw/TanSK2005}{{71}{2005}{{Tan et~al.}}{{Tan, Steinbach \& Kumar}}}
\bibcite{tweedie1947functions}{{72}{1947}{{Tweedie}}{{Tweedie}}}
\bibcite{Uhlenbeck1930}{{73}{1930}{{Uhlenbeck \& Ornstein}}{{Uhlenbeck \& Ornstein}}}
\bibcite{Vapnik1997}{{74}{1997}{{Vapnik}}{{Vapnik}}}
\bibcite{Vaswani2017}{{75}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser \& Polosukhin}}}
\bibcite{2020SciPy-NMeth}{{76}{2020}{{Virtanen et~al.}}{{Virtanen et~al.,}}}
\bibcite{Wang2023}{{77}{2023}{{Wang et~al.}}{{Wang, Zheng, He, Chen \& Zhou}}}
\bibcite{LinYang2023}{{78}{2023}{{Yang et~al.}}{{Yang et~al.,}}}
\bibcite{sdss}{{79}{2000}{{{York} et~al.}}{{{York} et~al.,}}}
\bibcite{Yu2015}{{80}{2015}{{{Yu} et~al.}}{{{Yu}, {Seff}, {Zhang}, {Song}, {Funkhouser} \& {Xiao}}}}
\bibcite{zhang2021diffusion}{{81}{2021}{{Zhang \& Chen}}{{Zhang \& Chen}}}
\bibcite{Zhao2023}{{82}{2023}{{{Zhao} et~al.}}{{{Zhao}, {Ting}, {Diao} \& {Mao}}}}
\newlabel{lastpage}{{4}{15}{Data Availability}{section.4}{}}
\gdef \@abspage@last{15}
