@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{Kingma2015,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{Vapnik1997,
author="Vapnik, Vladimir N.",
editor="Gerstner, Wulfram
and Germond, Alain
and Hasler, Martin
and Nicoud, Jean-Daniel",
title="The Support Vector method",
booktitle="Artificial Neural Networks --- ICANN'97",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="261--271",
abstract="The Support Vector (SV) method is a new general method of function estimation which does not depend explicitly on the dimensionality of input space. It was applied for pattern recognition, regression estimation, and density estimation problems as well as for problems of solving linear operator equations. In this article we describe the general idea of the SV method and present theorems demonstrating that the generalization ability of the SV method is based on factors which classical statistics do not take into account. We also describe the SV method for density estimation in a set of functions defined by a mixture of an infinite number of Gaussians.",
isbn="978-3-540-69620-9"
}



@ARTICLE{Lim2017,
       author = {{Lim}, Jae Hyun and {Ye}, Jong Chul},
        title = "{Geometric GAN}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = 2017,
        month = may,
          eid = {arXiv:1705.02894},
        pages = {arXiv:1705.02894},
          doi = {10.48550/arXiv.1705.02894},
archivePrefix = {arXiv},
       eprint = {1705.02894},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170502894L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Karras2018ASG,
  title={A Style-Based Generator Architecture for Generative Adversarial Networks},
  author={Tero Karras and Samuli Laine and Timo Aila},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
  pages={4396-4405},
  url={https://api.semanticscholar.org/CorpusID:54482423}
}

@INPROCEEDINGS{Karras2020,
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Analyzing and Improving the Image Quality of StyleGAN}, 
  year={2020},
  volume={},
  number={},
  pages={8107-8116},
  keywords={Generators;Training;Image resolution;Modulation;Convolution;Measurement;Standards},
  doi={10.1109/CVPR42600.2020.00813}}


@inproceedings{Brock2019,
  author       = {Andrew Brock and
                  Jeff Donahue and
                  Karen Simonyan},
  title        = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=B1xsqj09Fm},
  timestamp    = {Thu, 25 Jul 2019 13:03:18 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/BrockDS19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{Jozdani2022,
title = {A review and meta-analysis of Generative Adversarial Networks and their applications in remote sensing},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {108},
pages = {102734},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102734},
url = {https://www.sciencedirect.com/science/article/pii/S0303243422000605},
author = {Shahab Jozdani and Dongmei Chen and Darren Pouliot and Brian {Alan Johnson}},
keywords = {Remote sensing, Generative adversarial networks, GANs, Deep learning},
abstract = {Generative Adversarial Networks (GANs) are one of the most creative advances in Deep Learning (DL) in recent years. The Remote Sensing (RS) community has adopted GANs quickly, and reported successful use in a wide variety of applications. Given a sharp increase in research on GANs in the field of RS, there is a need for an in-depth review of the major technological/methodological advances and new applications. In this regard, we conducted a comprehensive review and meta-analysis of GAN-related RS papers, with the goals of familiarizing the RS community with the potential of GANs and helping researchers further explore RS applications of GANs by untangling challenges common in this field. Our review is based on 231 journal papers that were retrieved and selected through the Web of Science (WoS) database. We reviewed the theories, applications, and challenges of GANs, and highlighted the gaps to explore in future studies. Through the meta-analysis conducted in this study, we observed that image classification (especially urban mapping) has been the most popular application of GANs, potentially due to the wide availability of benchmark datasets. One the other hand, we found that relatively few studies have explored the potential of GANs for analyzing medium spatial-resolution multi-spectral images (e.g., Landsat or Sentinel-2), even though such images are often freely available and useful for a wide range of applications (e.g., urban expansion analysis, vegetation mapping, etc.). In spite of the applications of GANs for different RS processing tasks, there are still several gaps/questions in this field such as: 1) which GAN models/configurations are more suitable for different applications? 2) to what degree can GANs replace real RS data in different applications? Such gaps/questions can be appropriately addressed by, for example, conducting experimental studies on evaluating different GAN models for various RS applications to provide better insights into how/which GAN models can be best deployed. The meta-analysis results presented in this study could be helpful for RS researchers to know the opportunities of using GANs and understand how GANs contribute to the current challenges in different RS applications.}
}
@article{Saxena2021,
author = {Saxena, Divya and Cao, Jiannong},
title = {Generative Adversarial Networks (GANs): Challenges, Solutions, and Future Directions},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446374},
doi = {10.1145/3446374},
abstract = {Generative Adversarial Networks (GANs) is a novel class of deep generative models that has recently gained significant attention. GANs learn complex and high-dimensional distributions implicitly over images, audio, and data. However, there exist major challenges in training of GANs, i.e., mode collapse, non-convergence, and instability, due to inappropriate design of network architectre, use of objective function, and selection of optimization algorithm. Recently, to address these challenges, several solutions for better design and optimization of GANs have been investigated based on techniques of re-engineered network architectures, new objective functions, and alternative optimization algorithms. To the best of our knowledge, there is no existing survey that has particularly focused on the broad and systematic developments of these solutions. In this study, we perform a comprehensive survey of the advancements in GANs design and optimization solutions proposed to handle GANs challenges. We first identify key research issues within each design and optimization technique and then propose a new taxonomy to structure solutions by key research issues. In accordance with the taxonomy, we provide a detailed discussion on different GANs variants proposed within each solution and their relationships. Finally, based on the insights gained, we present promising research directions in this rapidly growing field.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {63},
numpages = {42},
keywords = {mode collapse, deep Generative models, computer vision, Image generation, Generative Adversarial Networks, GANs variants, GANs challenges, GANs applications, GANs Survey, GANs, Deep learning}
}

@inproceedings{liu2021towards,
title={Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis},
author={Bingchen Liu and Yizhe Zhu and Kunpeng Song and Ahmed Elgammal},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=1Fqg133qRaI}
}
@inproceedings{Gulrajani2017,
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
title = {Improved training of wasserstein GANs},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5769–5779},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}
@article{Schawinski2017,
    author = {Schawinski, Kevin and Zhang, Ce and Zhang, Hantian and Fowler, Lucas and Santhanam, Gokula Krishnan},
    title = "{Generative adversarial networks recover features in astrophysical images of galaxies beyond the deconvolution limit}",
    journal = {Monthly Notices of the Royal Astronomical Society: Letters},
    volume = {467},
    number = {1},
    pages = {L110-L114},
    year = {2017},
    month = {01},
    abstract = "{Observations of astrophysical objects such as galaxies are limited by various sources of random and systematic noise from the sky background, the optical system of the telescope and the detector used to record the data. Conventional deconvolution techniques are limited in their ability to recover features in imaging data by the Shannon–Nyquist sampling theorem. Here, we train a generative adversarial network (GAN) on a sample of 4550 images of nearby galaxies at 0.01 \&lt; z \&lt; 0.02 from the Sloan Digital Sky Survey and conduct 10× cross-validation to evaluate the results. We present a method using a GAN trained on galaxy images that can recover features from artificially degraded images with worse seeing and higher noise than the original with a performance that far exceeds simple deconvolution. The ability to better recover detailed features such as galaxy morphology from low signal to noise and low angular resolution imaging data significantly increases our ability to study existing data sets of astrophysical objects as well as future observations with observatories such as the Large Synoptic Sky Telescope (LSST) and the Hubble and James Webb space telescopes.}",
    issn = {1745-3925},
    doi = {10.1093/mnrasl/slx008},
    url = {https://doi.org/10.1093/mnrasl/slx008},
    eprint = {https://academic.oup.com/mnrasl/article-pdf/467/1/L110/56944790/mnrasl\_467\_1\_l110.pdf},
}





@ARTICLE{Coccomini2021,
       author = {{Coccomini}, Davide and {Messina}, Nicola and {Gennaro}, Claudio and {Falchi}, Fabrizio},
        title = "{Generative Adversarial Networks for Astronomical Images Generation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
         year = 2021,
        month = nov,
          eid = {arXiv:2111.11578},
        pages = {arXiv:2111.11578},
          doi = {10.48550/arXiv.2111.11578},
archivePrefix = {arXiv},
       eprint = {2111.11578},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv211111578C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@book{books/aw/TanSK2005,
  added-at = {2021-11-08T00:00:00.000+0100},
  author = {Tan, Pang-Ning and Steinbach, Michael S. and Kumar, Vipin},
  biburl = {https://www.bibsonomy.org/bibtex/2d981d131977639fbe1e5d195e3b4060e/dblp},
  ee = {http://www-users.cs.umn.edu/~kumar/dmbook/},
  interhash = {4937668769a7962cb48bf7195dcad387},
  intrahash = {d981d131977639fbe1e5d195e3b4060e},
  isbn = {0-321-32136-7},
  keywords = {dblp},
  publisher = {Addison-Wesley},
  timestamp = {2024-04-09T07:57:23.000+0200},
  title = {Introduction to Data Mining},
  year = 2005
}



@article{corner2016,
  doi = {10.21105/joss.00024},
  url = {https://doi.org/10.21105/joss.00024},
  year  = {2016},
  month = {jun},
  publisher = {The Open Journal},
  volume = {1},
  number = {2},
  pages = {24},
  author = {Daniel Foreman-Mackey},
  title = {corner.py: Scatterplot matrices in Python},
  journal = {The Journal of Open Source Software}
}

@article{10.1093/mnras/stv2078,
    author = {Snyder, Gregory F. and Torrey, Paul and Lotz, Jennifer M. and Genel, Shy and McBride, Cameron K. and Vogelsberger, Mark and Pillepich, Annalisa and Nelson, Dylan and Sales, Laura V. and Sijacki, Debora and Hernquist, Lars and Springel, Volker},
    title = "{Galaxy morphology and star formation in the Illustris Simulation at z = 0}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {454},
    number = {2},
    pages = {1886-1908},
    year = {2015},
    month = {10},
    issn = {0035-8711},
    doi = {10.1093/mnras/stv2078},
    url = {https://doi.org/10.1093/mnras/stv2078},
    eprint = {https://academic.oup.com/mnras/article-pdf/454/2/1886/13769809/stv2078.pdf},
}





@ARTICLE{2004AJ....128..163L,
       author = {{Lotz}, Jennifer M. and {Primack}, Joel and {Madau}, Piero},
        title = "{A New Nonparametric Approach to Galaxy Morphological Classification}",
      journal = {\aj},
     keywords = {Galaxies: Fundamental Parameters, Galaxies: High-Redshift, Galaxies: Peculiar, Galaxies: Structure, Astrophysics},
         year = 2004,
        month = jul,
       volume = {128},
       number = {1},
        pages = {163-182},
          doi = {10.1086/421849},
archivePrefix = {arXiv},
       eprint = {astro-ph/0311352},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004AJ....128..163L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{2003ApJS..147....1C,
       author = {{Conselice}, Christopher J.},
        title = "{The Relationship between Stellar Light Distributions of Galaxies and Their Formation Histories}",
      journal = {\apjs},
     keywords = {Galaxies: Evolution, Galaxies: Formation, Galaxies: Structure, Astrophysics},
         year = 2003,
        month = jul,
       volume = {147},
       number = {1},
        pages = {1-28},
          doi = {10.1086/375001},
archivePrefix = {arXiv},
       eprint = {astro-ph/0303065},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2003ApJS..147....1C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2000AJ....119.2645B,
       author = {{Bershady}, Matthew A. and {Jangren}, Anna and {Conselice}, Christopher J.},
        title = "{Structural and Photometric Classification of Galaxies. I. Calibration Based on a Nearby Galaxy Sample}",
      journal = {\aj},
     keywords = {Galaxies: Compact, Galaxies: Fundamental Parameters, Galaxies: Starburst, Astrophysics},
         year = 2000,
        month = jun,
       volume = {119},
       number = {6},
        pages = {2645-2663},
          doi = {10.1086/301386},
archivePrefix = {arXiv},
       eprint = {astro-ph/0002262},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2000AJ....119.2645B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2019MNRAS.483.4140R,
       author = {{Rodriguez-Gomez}, Vicente and {Snyder}, Gregory F. and {Lotz}, Jennifer M. and {Nelson}, Dylan and {Pillepich}, Annalisa and {Springel}, Volker and {Genel}, Shy and {Weinberger}, Rainer and {Tacchella}, Sandro and {Pakmor}, R{\"u}diger and {Torrey}, Paul and {Marinacci}, Federico and {Vogelsberger}, Mark and {Hernquist}, Lars and {Thilker}, David A.},
        title = "{The optical morphologies of galaxies in the IllustrisTNG simulation: a comparison to Pan-STARRS observations}",
      journal = {\mnras},
     keywords = {methods: numerical, techniques: image processing, galaxies: formation, galaxies: statistics, galaxies: structure, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = 2019,
        month = mar,
       volume = {483},
       number = {3},
        pages = {4140-4159},
          doi = {10.1093/mnras/sty3345},
archivePrefix = {arXiv},
       eprint = {1809.08239},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.4140R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@ARTICLE{PyTorch2019,
       author = {{Paszke}, Adam and {Gross}, Sam and {Massa}, Francisco and {Lerer}, Adam and {Bradbury}, James and {Chanan}, Gregory and {Killeen}, Trevor and {Lin}, Zeming and {Gimelshein}, Natalia and {Antiga}, Luca and {Desmaison}, Alban and {K{\"o}pf}, Andreas and {Yang}, Edward and {DeVito}, Zach and {Raison}, Martin and {Tejani}, Alykhan and {Chilamkurthy}, Sasank and {Steiner}, Benoit and {Fang}, Lu and {Bai}, Junjie and {Chintala}, Soumith},
        title = "{PyTorch: An Imperative Style, High-Performance Deep Learning Library}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
         year = 2019,
        month = dec,
          eid = {arXiv:1912.01703},
        pages = {arXiv:1912.01703},
          doi = {10.48550/arXiv.1912.01703},
archivePrefix = {arXiv},
       eprint = {1912.01703},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191201703P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{Wang2023,
title={Diffusion-{GAN}: Training {GAN}s with Diffusion},
author={Zhendong Wang and Huangjie Zheng and Pengcheng He and Weizhu Chen and Mingyuan Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=HZf7UbpWHuA}
}


@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@article{Remy2023,
	author = {{Remy, B.} and {Lanusse, F.} and {Jeffrey, N.} and {Liu, J.} and {Starck, J.-L.} and {Osato, K.} and {Schrabback, T.}},
	title = {Probabilistic mass-mapping with neural score estimation},
	DOI= "10.1051/0004-6361/202243054",
	url= "https://doi.org/10.1051/0004-6361/202243054",
	journal = {\aap},
	year = 2023,
	volume = 672,
	pages = "A51",
}


@ARTICLE{Schanz2023,
       author = {{Schanz}, Andreas and {List}, Florian and {Hahn}, Oliver},
        title = "{Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
         year = 2023,
        month = oct,
          eid = {arXiv:2310.06929},
        pages = {arXiv:2310.06929},
          doi = {10.48550/arXiv.2310.06929},
archivePrefix = {arXiv},
       eprint = {2310.06929},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv231006929S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{Zhao2023,
       author = {{Zhao}, Xiaosheng and {Ting}, Yuan-Sen and {Diao}, Kangning and {Mao}, Yi},
        title = "{Can diffusion model conditionally generate astrophysical images?}",
      journal = {\mnras},
     keywords = {methods: statistical, dark ages, reionization, first stars, early Universe, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = 2023,
        month = dec,
       volume = {526},
       number = {2},
        pages = {1699-1712},
          doi = {10.1093/mnras/stad2778},
archivePrefix = {arXiv},
       eprint = {2307.09568},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023MNRAS.526.1699Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{zhang2021diffusion,
title={Diffusion Normalizing Flow},
author={Qinsheng Zhang and Yongxin Chen},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=x1Lp2bOlVIo}
}

@inproceedings{gong2021interpreting,
title={Interpreting diffusion score matching using normalizing flow},
author={Wenbo Gong and Yingzhen Li},
booktitle={ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models},
year={2021},
url={https://openreview.net/forum?id=jxsmOXCDv9l}
}

@InProceedings{Chen2018,
  title = 	 {{P}ixel{SNAIL}: An Improved Autoregressive Generative Model},
  author =       {Chen, XI and Mishra, Nikhil and Rohaninejad, Mostafa and Abbeel, Pieter},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {864--872},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/chen18h/chen18h.pdf},
  url = 	 {https://proceedings.mlr.press/v80/chen18h.html},
  abstract = 	 {Autoregressive generative models achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this paper, we describe the resulting model and present state-of-the-art log-likelihood results on heavily benchmarked datasets: CIFAR-10, $32 \times 32$ ImageNet and $64 \times 64$ ImageNet. Our implementation will be made available at \url{https://github.com/neocxi/pixelsnail-public}.}
}

@InProceedings{Germain2015,
  title = 	 {MADE: Masked Autoencoder for Distribution Estimation},
  author = 	 {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {881--889},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/germain15.html},
  abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}


@article{Crenshaw_2024,
doi = {10.3847/1538-3881/ad54bf},
url = {https://dx.doi.org/10.3847/1538-3881/ad54bf},
year = {2024},
month = {jul},
publisher = {The American Astronomical Society},
volume = {168},
number = {2},
pages = {80},
author = {John Franklin Crenshaw and J. Bryce Kalmbach and Alexander Gagliano and Ziang Yan and Andrew J. Connolly and Alex I. Malz and Samuel J. Schmidt and The LSST Dark Energy Science Collaboration},
title = {Probabilistic Forward Modeling of Galaxy Catalogs with Normalizing Flows},
journal = {The Astronomical Journal},
abstract = {Evaluating the accuracy and calibration of the redshift posteriors produced by photometric redshift (photo-z) estimators is vital for enabling precision cosmology and extragalactic astrophysics with modern wide-field photometric surveys. Evaluating photo-z posteriors on a per-galaxy basis is difficult, however, as real galaxies have a true redshift but not a true redshift posterior. We introduce PZFlow, a Python package for the probabilistic forward modeling of galaxy catalogs with normalizing flows. For catalogs simulated with PZFlow, there is a natural notion of “true” redshift posteriors that can be used for photo-z validation. We use PZFlow to simulate a photometric galaxy catalog where each galaxy has a redshift, noisy photometry, shape information, and a true redshift posterior. We also demonstrate the use of an ensemble of normalizing flows for photo-z estimation. We discuss how PZFlow will be used to validate the photo-z estimation pipeline of the Dark Energy Science Collaboration, and the wider applicability of PZFlow for statistical modeling of any tabular data.}
}



@inproceedings{Papamakarios2017a,
author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
title = {Masked autoregressive flow for density estimation},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {2335–2344},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@inproceedings{Vaswani2017,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{tweedie1947functions, 
title={Functions of a statistical variate with given means, with special reference to Laplacian distributions}, 
volume={43}, 
DOI={10.1017/S0305004100023185}, 
number={1}, 
journal={Mathematical Proceedings of the Cambridge Philosophical Society}, author={Tweedie, M. C. K.}, 
year={1947}, 
pages={41–49}
}
 
@inproceedings{huang2019solving,
title={Solving {ODE} with Universal Flows: Approximation Theory for Flow-Based Models},
author={Chin-Wei Huang and Laurent Dinh and Aaron Courville},
booktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},
year={2019},
url={https://openreview.net/forum?id=cfKpOiUzF}
}

@ARTICLE{Bogachev2005,
       author = {{Bogachev}, V.~I. and {Kolesnikov}, A.~V. and {Medvedev}, K.~V.},
        title = "{Triangular transformations of measures}",
      journal = {Sbornik: Mathematics},
         year = 2005,
        month = apr,
       volume = {196},
       number = {3},
        pages = {309-335},
          doi = {10.1070/SM2005v196n03ABEH000882},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2005SbMat.196..309B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Papamakarios2021,
  author  = {George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
  title   = {Normalizing Flows for Probabilistic Modeling and Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {57},
  pages   = {1--64},
  url     = {http://jmlr.org/papers/v22/19-1028.html}
}

@article{weng2018flow,
  title   = "Flow-based Deep Generative Models",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-10-13-flow-models/"
}

@article{Tabak2010,
author = {Esteban G. Tabak and Eric Vanden-Eijnden},
title = {{Density estimation by dual ascent of the log-likelihood}},
volume = {8},
journal = {Communications in Mathematical Sciences},
number = {1},
publisher = {International Press of Boston},
pages = {217 -- 233},
keywords = {Density estimation, machine learning, maximum likelihood},
year = {2010},
}

@article{Tabak2013a,
author = {Tabak, E. G. and Turner, Cristina V.},
title = {A Family of Nonparametric Density Estimation Algorithms},
journal = {Communications on Pure and Applied Mathematics},
volume = {66},
number = {2},
pages = {145-164},
doi = {https://doi.org/10.1002/cpa.21423},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21423},
abstract = {Abstract A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. © 2012 Wiley Periodicals, Inc.},
year = {2013}
}


@inproceedings{Rezende2015,
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
title = {Variational inference with normalizing flows},
year = {2015},
publisher = {JMLR.org},
abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1530–1538},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@ARTICLE{2014arXiv1410.8516D,
       author = {{Dinh}, Laurent and {Krueger}, David and {Bengio}, Yoshua},
        title = "{NICE: Non-linear Independent Components Estimation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2014,
        month = oct,
          eid = {arXiv:1410.8516},
        pages = {arXiv:1410.8516},
archivePrefix = {arXiv},
       eprint = {1410.8516},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1410.8516D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{DinhKB14,
  author       = {Laurent Dinh and
                  David Krueger and
                  Yoshua Bengio},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {{NICE:} Non-linear Independent Components Estimation},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1410.8516},
  timestamp    = {Fri, 02 Aug 2024 11:44:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/DinhKB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{2016arXiv160508803D,
       author = {{Dinh}, Laurent and {Sohl-Dickstein}, Jascha and {Bengio}, Samy},
        title = "{Density estimation using Real NVP}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2016,
        month = may,
          eid = {arXiv:1605.08803},
        pages = {arXiv:1605.08803},
archivePrefix = {arXiv},
       eprint = {1605.08803},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160508803D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{DinhSB17,
  author       = {Laurent Dinh and
                  Jascha Sohl{-}Dickstein and
                  Samy Bengio},
  title        = {Density estimation using Real {NVP}},
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017,
                  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2017},
  url          = {https://openreview.net/forum?id=HkpbnH9lx},
  timestamp    = {Thu, 25 Jul 2019 14:25:58 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DinhSB17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{brock2018large,
title={Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
author={Andrew Brock and Jeff Donahue and Karen Simonyan},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1xsqj09Fm},
}

@inproceedings{engel2018latent,
title={Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models},
author={Jesse Engel and Matthew Hoffman and Adam Roberts},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Sy8XvGb0-},
}

@article{Takida2022,
author = {Takida, Yuhta and Liao, Wei-Hsiang and Lai, Chieh-Hsin and Uesaka, Toshimitsu and Takahashi, Shusuke and Mitsufuji, Yuki},
title = {Preventing oversmoothing in VAE via generalized variance parameterization},
year = {2022},
issue_date = {Oct 2022},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {509},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2022.08.067},
doi = {10.1016/j.neucom.2022.08.067},
journal = {Neurocomput.},
month = {oct},
pages = {137–156},
numpages = {20},
keywords = {Maximum likelihood estimation, Decoder variance, Posterior collapse, Variational autoencoders, Gaussian model, Bayesian inference}
}

@article{JMLR:v21:19-047,
  author  = {Mathieu Andreux and Tomás Angles and Georgios Exarchakis and Roberto Leonarduzzi and Gaspar Rochette and Louis Thiry and John Zarka and Stéphane Mallat and Joakim Andén and Eugene Belilovsky and Joan Bruna and Vincent Lostanlen and Muawiz Chaudhary and Matthew J. Hirn and Edouard Oyallon and Sixin Zhang and Carmine Cella and Michael Eickenberg},
  title   = {Kymatio: Scattering Transforms in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {60},
  pages   = {1--6},
  url     = {http://jmlr.org/papers/v21/19-047.html}
}

@article{Unser2013,
author = {Unser, Michael and Chenouard, Nicolas},
title = {A Unifying Parametric Framework for 2D Steerable Wavelet Transforms},
journal = {SIAM Journal on Imaging Sciences},
volume = {6},
number = {1},
pages = {102-135},
year = {2013},
doi = {10.1137/120866014},
URL = {https://doi.org/10.1137/120866014},
eprint = {https://doi.org/10.1137/120866014},
abstract = { We introduce a complete parameterization of the family of two-dimensional steerable wavelets that are polar-separable in the Fourier domain under the constraint of self-reversibility. These wavelets are constructed by multiorder generalized Riesz transformation of a primary isotropic bandpass pyramid. The backbone of the transform (pyramid) is characterized by a radial frequency profile function \$h(\omega)\$, while the directional wavelet components at each scale are encoded by an \$M \times (2N+1)\$ shaping matrix \${\bf U}\$, where \$M\$ is the number of wavelet channels and \$N\$ the order of the Riesz transform. We provide general conditions on \$h(\omega)\$ and \${\bf U}\$ for the underlying wavelet system to form a tight frame of \$L\_2(\mathbb{R}^2)\$ (with a redundancy factor \$4/3M\$). The proposed framework ensures that the wavelets are steerable and provides new degrees of freedom (shaping matrix \${\bf U}\$) that can be exploited for designing specific wavelet systems. It encompasses many known transforms as particular cases: Simoncelli's steerable pyramid, Marr gradient and Hessian wavelets, monogenic wavelets, and \$N\$th-order Riesz and circular harmonic wavelets. We take advantage of the framework to construct new generalized spheroidal prolate wavelets, whose angular selectivity is maximized, as well as signal-adapted detectors based on principal component analysis. We also introduce a curvelet-like steerable wavelet system. Finally, we illustrate the advantages of some of the designs for signal denoising, feature extraction, pattern analysis, and source separation. }
}





@inproceedings{Mohan2020Robust,
title={Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks},
author={Sreyas Mohan and Zahra Kadkhodaie and Eero P. Simoncelli and Carlos Fernandez-Granda},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJlSmC4FPS}
}

@Article{sdss,
  author	= {{York}, D.~G. and {Adelman}, J. and {Anderson}, J.~E., Jr.
		  and {Anderson}, S.~F. and {Annis}, J. and {Bahcall}, N.~A.
		  and {Bakken}, J.~A. and {Barkhouser}, R. and {Bastian}, S.
		  and {Berman}, E. and others},
  title		= "{The Sloan Digital Sky Survey: Technical Summary}",
  journal	= {\aj},
  keywords	= {Cosmology: Observations, Instrumentation: Miscellaneous,
		  Astrophysics},
  year		= 2000,
  month		= sep,
  volume	= {120},
  number	= {3},
  pages		= {1579-1587},
  doi		= {10.1086/301513},
  archiveprefix	= {arXiv},
  eprint	= {astro-ph/0006396},
  primaryclass	= {astro-ph},
  adsurl	= {https://ui.adsabs.harvard.edu/abs/2000AJ....120.1579Y},
  adsnote	= {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{sdssdr7,
  doi		= {10.1088/0067-0049/182/2/543},
  url		= {https://doi.org/10.1088/0067-0049/182/2/543},
  year		= 2009,
  month		= {may},
  publisher	= {American Astronomical Society},
  volume	= {182},
  number	= {2},
  pages		= {543--558},
  author	= {Kevork N. Abazajian and Jennifer K. Adelman-McCarthy and
		  Marcel A. Agüeros and Sahar S. Allam and Carlos Allende
		  Prieto and Deokkeun An and Kurt S. J. Anderson and Scott F.
		  Anderson and James Annis and Neta A. Bahcall and C. A. L.
		  Bailer-Jones and J. C. Barentine and Bruce A. Bassett and
		  Andrew C. Becker and Timothy C. Beers and Eric F. Bell and
		  Vasily Belokurov and Andreas A. Berlind and Eileen F.
		  Berman and Mariangela Bernardi and Steven J. Bickerton and
		  Dmitry Bizyaev and John P. Blakeslee and Michael R. Blanton
		  and John J. Bochanski and William N. Boroski and Howard J.
		  Brewington and Jarle Brinchmann and J. Brinkmann and Robert
		  J. Brunner and Tam{\'{a}}s Budav{\'{a}}ri and Larry N.
		  Carey and Samuel Carliles and Michael A. Carr and Francisco
		  J. Castander and David Cinabro and A. J. Connolly and
		  Istv{\'{a}}n Csabai and Carlos E. Cunha and Paul C.
		  Czarapata and James R. A. Davenport and Ernst de Haas and
		  Ben Dilday and Mamoru Doi and Daniel J. Eisenstein and
		  Michael L. Evans and N. W. Evans and Xiaohui Fan and Scott
		  D. Friedman and Joshua A. Frieman and Masataka Fukugita and
		  Boris T. Gänsicke and Evalyn Gates and Bruce Gillespie and
		  G. Gilmore and Belinda Gonzalez and Carlos F. Gonzalez and
		  Eva K. Grebel and James E. Gunn and Zsuzsanna Györy and
		  Patrick B. Hall and Paul Harding and Frederick H. Harris
		  and Michael Harvanek and Suzanne L. Hawley and Jeffrey J.
		  E. Hayes and Timothy M. Heckman and John S. Hendry and
		  Gregory S. Hennessy and Robert B. Hindsley and J. Hoblitt
		  and Craig J. Hogan and David W. Hogg and Jon A. Holtzman
		  and Joseph B. Hyde and Shin-ichi Ichikawa and Takashi
		  Ichikawa and Myungshin Im and {\v{Z}}eljko Ivezi{\'{c}} and
		  Sebastian Jester and Linhua Jiang and Jennifer A. Johnson
		  and Anders M. Jorgensen and Mario Juri{\'{c}} and Stephen
		  M. Kent and R. Kessler and S. J. Kleinman and G. R. Knapp
		  and Kohki Konishi and Richard G. Kron and Jurek Krzesinski
		  and Nikolay Kuropatkin and Hubert Lampeitl and Svetlana
		  Lebedeva and Myung Gyoon Lee and Young Sun Lee and R.
		  French Leger and S{\'{e}}bastien L{\'{e}}pine and Nolan Li
		  and Marcos Lima and Huan Lin and Daniel C. Long and Craig
		  P. Loomis and Jon Loveday and Robert H. Lupton and Eugene
		  Magnier and Olena Malanushenko and Viktor Malanushenko and
		  Rachel Mandelbaum and Bruce Margon and John P. Marriner and
		  David Mart{\'{\i}}nez-Delgado and Takahiko Matsubara and
		  Peregrine M. McGehee and Timothy A. McKay and Avery Meiksin
		  and Heather L. Morrison and Fergal Mullally and Jeffrey A.
		  Munn and Tara Murphy and Thomas Nash and Ada Nebot and Eric
		  H. Neilsen and Heidi Jo Newberg and Peter R. Newman and
		  Robert C. Nichol and Tom Nicinski and Maria
		  Nieto-Santisteban and Atsuko Nitta and Sadanori Okamura and
		  Daniel J. Oravetz and Jeremiah P. Ostriker and Russell Owen
		  and Nikhil Padmanabhan and Kaike Pan and Changbom Park and
		  George Pauls and John Peoples and Will J. Percival and
		  Jeffrey R. Pier and Adrian C. Pope and Dimitri Pourbaix and
		  Paul A. Price and Norbert Purger and Thomas Quinn and M.
		  Jordan Raddick and Paola Re Fiorentin and Gordon T.
		  Richards and Michael W. Richmond and Adam G. Riess and
		  Hans-Walter Rix and Constance M. Rockosi and Masao Sako and
		  David J. Schlegel and Donald P. Schneider and Ralf-Dieter
		  Scholz and Matthias R. Schreiber and Axel D. Schwope and
		  Uro{\v{s}} Seljak and Branimir Sesar and Erin Sheldon and
		  Kazu Shimasaku and Valena C. Sibley and A. E. Simmons and
		  Thirupathi Sivarani and J. Allyn Smith and Martin C. Smith
		  and Vernesa Smol{\v{c}}i{\'{c}} and Stephanie A. Snedden
		  and Albert Stebbins and Matthias Steinmetz and Chris
		  Stoughton and Michael A. Strauss and Mark SubbaRao and
		  Yasushi Suto and Alexander S. Szalay and Istv{\'{a}}n
		  Szapudi and Paula Szkody and Masayuki Tanaka and Max
		  Tegmark and Luis F. A. Teodoro and Aniruddha R. Thakar and
		  Christy A. Tremonti and Douglas L. Tucker and Alan Uomoto
		  and Daniel E. Vanden Berk and Jan Vandenberg and S. Vidrih
		  and Michael S. Vogeley and Wolfgang Voges and Nicole P.
		  Vogt and Yogesh Wadadekar and Shannon Watters and David H.
		  Weinberg and Andrew A. West and Simon D. M. White and Brian
		  C. Wilhite and Alainna C. Wonders and Brian Yanny and D. R.
		  Yocum and Donald G. York and Idit Zehavi and Stefano
		  Zibetti and Daniel B. Zucker},
  title		= {THE SEVENTH DATA RELEASE OF THE SLOAN DIGITAL SKY SURVEY},
  journal	= {The Astrophysical Journal Supplement Series}
}

@misc{mandelbaum_2019_3242143,
  author       = {Mandelbaum, Rachel and
                  Lackner, Claire and
                  Leauthaud, Alexie and
                  Rowe, Barnaby},
  title        = {COSMOS real galaxy dataset},
  month        = jul,
  year         = 2019,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.3242143},
  url          = {https://doi.org/10.5281/zenodo.3242143}
}

@INPROCEEDINGS{Hataya2023,
author = {R. Hataya and H. Bao and H. Arai},
booktitle = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
title = {Will Large-scale Generative Models Corrupt Future Datasets?},
year = {2023},
volume = {},
issn = {},
pages = {20498-20508},
abstract = {Recently proposed large-scale text-to-image generative models such as DALL•E 2 [47], Midjourney [42], and StableDiffusion [51] can generate high-quality and realistic images from users’ prompts. Not limited to the research community, ordinary Internet users enjoy these generative models, and consequently, a tremendous amount of generated images have been shared on the Internet. Meanwhile, today’s success of deep learning in the computer vision field owes a lot to images collected from the Internet. These trends lead us to a research question: &quot;will such generated images impact the quality of future datasets and the performance of computer vision models positively or negatively?&quot; This paper empirically answers this question by simulating contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using a state-of-the-art generative model and evaluate models trained with &quot;contaminated&quot; datasets on various tasks, including image classification and image generation. Throughout experiments, we conclude that generated images negatively affect downstream performance, while the significance depends on tasks and the amount of generated images. The generated datasets and the codes for experiments will be publicly released for future research. Generated datasets and source codes are available from https://github.com/moskomule/dataset-contamination.},
keywords = {computer vision;image synthesis;computational modeling;watermarking;market research;information filters;data models},
doi = {10.1109/ICCV51070.2023.01879},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV51070.2023.01879},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}



@conference{ravanbakhsh2016,
author = {S. Ravanbakhsh and F. Lanusse and R. Mandelbaum and J. Schneider and B. Poczos},
title = {Enabling Dark Energy Science with Deep Generative Models of Galaxy Images},
booktitle = {Proceedings of 31st AAAI Conference on Artificial Intelligence (AAAI '17)},
year = {2017},
month = {February},
pages = {1488 - 1494},
eid		= {arXiv:1609.05796},
archiveprefix	= {arXiv},
eprint	= {1609.05796},
}




@article{smith2021,
    author = {Smith, Michael J and Geach, James E and Jackson, Ryan A and Arora, Nikhil and Stone, Connor and Courteau, Stéphane},
    title = "{Realistic galaxy image simulation via score-based generative models}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {511},
    number = {2},
    pages = {1808-1818},
    year = {2022},
    month = {01},
    abstract = "{We show that a denoising diffusion probabilistic model (DDPM), a class of score-based generative model, can be used to produce realistic mock images that mimic observations of galaxies. Our method is tested with Dark Energy Spectroscopic Instrument (DESI) grz imaging of galaxies from the Photometry and Rotation curve OBservations from Extragalactic Surveys (PROBES) sample and galaxies selected from the Sloan Digital Sky Survey. Subjectively, the generated galaxies are highly realistic when compared with samples from the real data set. We quantify the similarity by borrowing from the deep generative learning literature, using the ‘Fréchet inception distance’ to test for subjective and morphological similarity. We also introduce the ‘synthetic galaxy distance’ metric to compare the emergent physical properties (such as total magnitude, colour, and half-light radius) of a ground truth parent and synthesized child data set. We argue that the DDPM approach produces sharper and more realistic images than other generative methods such as adversarial networks (with the downside of more costly inference), and could be used to produce large samples of synthetic observations tailored to a specific imaging survey. We demonstrate two potential uses of the DDPM: (1) accurate inpainting of occluded data, such as satellite trails, and (2) domain transfer, where new input images can be processed to mimic the properties of the DDPM training set. Here we ‘DESI-fy’ cartoon images as a proof of concept for domain transfer. Finally, we suggest potential applications for score-based approaches that could motivate further research on this topic within the astronomical community.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stac130},
    url = {https://doi.org/10.1093/mnras/stac130},
    eprint = {https://academic.oup.com/mnras/article-pdf/511/2/1808/47158945/stac130.pdf},
}





@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


@inproceedings{Kingma2014,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  added-at = {2020-10-15T14:36:56.000+0200},
  author = {Kingma, Diederik P. and Welling, Max},
  biburl = {https://www.bibsonomy.org/bibtex/242e5be6faa01cba2587f4907ac99dce8/annakrause},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {http://arxiv.org/abs/1312.6114},
  eprintclass = {stat.ML},
  eprinttype = {arXiv},
  interhash = {a626a9d77a123c52405a08da983203cb},
  intrahash = {42e5be6faa01cba2587f4907ac99dce8},
  keywords = {cs.LG stat.ML vae},
  timestamp = {2021-02-01T17:13:18.000+0100},
  title = {{Auto-Encoding Variational Bayes}},
  year = 2014
}


@inproceedings{janulewicz2024assessing,
title={Assessing the Viability of Generative Modeling in Simulated Astronomical Observations},
author={Patrick Janulewicz and Laurence Perreault-Levasseur and Tracy Webb},
booktitle={ICML 2024 Workshop on Structured Probabilistic Inference {\&} Generative Modeling},
year={2024},
url={https://openreview.net/forum?id=lzLMJ6KkiS}
}

@article{HACKSTEIN2023100685,
title = {Evaluation metrics for galaxy image generators},
journal = {Astronomy and Computing},
volume = {42},
pages = {100685},
year = {2023},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2022.100685},
url = {https://www.sciencedirect.com/science/article/pii/S2213133722000993},
author = {S. Hackstein and V. Kinakh and C. Bailer and M. Melchior},
keywords = {Deep learning, Generative models, Computer-vision, Evaluation, Galaxy morphology},
abstract = {A major problem with deep generative models is verifying that the generated distribution resembles the target distribution while the individual generated sample is indistinguishable from the original data. In particular, for application in astrophysics we need to be sure that the generated data matches our prior knowledge and that the generated samples entail all object types with the correct frequency and diversity. We currently lack objective ways to systematically assess these quality aspects, where human inspection reaches its limits, as this requires detailed analysis of a large data volume. In this work, we identify reasonable metrics for the quality of galaxy image generators. To this end, we compare a small set of conditional image generators, trained on galaxy images with classification labels for visual morphology features. Our main contribution is a new set of cluster-based metrics for matching the generated distribution to the target distribution. Furthermore, we use the Wasserstein distance on proxies for galaxy morphology as well as a number of other metrics commonly used for image generators. The newly introduced cluster-based metrics are good proxies for the quality of the generated distribution and are suited for automatized identification of mode collapse. Furthermore, the cluster metrics allow for a qualitative interpretation of the generated distribution. The metrics based on morphological statistics provide a useful tool to probe the physical soundness of generated samples. Finally, we find that kernel inception distance used with an InceptionV3 model pre-trained on ImageNet is a good proxy for the overall quality of galaxy image generators, although it cannot be interpreted that easily.}
}

@article{PhysRevD.107.076017,
  title = {Evaluating generative models in high energy physics},
  author = {Kansal, Raghav and Li, Anni and Duarte, Javier and Chernyavskaya, Nadezda and Pierini, Maurizio and Orzari, Breno and Tomei, Thiago},
  journal = {Phys. Rev. D},
  volume = {107},
  issue = {7},
  pages = {076017},
  numpages = {18},
  year = {2023},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.107.076017},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.107.076017}
}


@article{Arcelin2020,
    author = {Arcelin, Bastien and Doux, Cyrille and Aubourg, Eric and Roucelle, Cécile and (The LSST Dark Energy Science Collaboration)},
    title = "{Deblending galaxies with variational autoencoders: A joint multiband, multi-instrument approach}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {500},
    number = {1},
    pages = {531-547},
    year = {2020},
    month = {10},
    abstract = "{Blending of galaxies has a major contribution in the systematic error budget of weak-lensing studies, affecting photometric and shape measurements, particularly for ground-based, deep, photometric galaxy surveys, such as the Rubin Observatory Legacy Survey of Space and Time (LSST). Existing deblenders mostly rely on analytic modelling of galaxy profiles and suffer from the lack of flexible yet accurate models. We propose to use generative models based on deep neural networks, namely variational autoencoders (VAE), to learn probabilistic models directly from data. We train a VAE on images of centred, isolated galaxies, which we reuse, as a prior, in a second VAE-like neural network in charge of deblending galaxies. We train our networks on simulated images including six LSST bandpass filters and the visible and near-infrared bands of the Euclid satellite, as our method naturally generalizes to multiple bands and can incorporate data from multiple instruments. We obtain median reconstruction errors on ellipticities and r-band magnitude between ±0.01 and ±0.05, respectively, in most cases, and ellipticity multiplicative bias of 1.6 per cent for blended objects in the optimal configuration. We also study the impact of decentring and prove the method to be robust. This method only requires the approximate centre of each target galaxy, but no assumptions about the number of surrounding objects, pointing to an iterative detection/deblending procedure we leave for future work. Finally, we discuss future challenges about training on real data and obtain encouraging results when applying transfer learning.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/staa3062},
    url = {https://doi.org/10.1093/mnras/staa3062},
    eprint = {https://academic.oup.com/mnras/article-pdf/500/1/531/34292544/staa3062.pdf},
}



@article{ROWE2015121,
title = {GalSim: The modular galaxy image simulation toolkit},
journal = {Astronomy and Computing},
volume = {10},
pages = {121-150},
year = {2015},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2015.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S221313371500013X},
author = {B.T.P. Rowe and M. Jarvis and R. Mandelbaum and G.M. Bernstein and J. Bosch and M. Simet and J.E. Meyers and T. Kacprzak and R. Nakajima and J. Zuntz and H. Miyatake and J.P. Dietrich and R. Armstrong and P. Melchior and M.S.S. Gill},
keywords = {Methods: data analysis, Techniques: image processing, Gravitational lensing, Cosmology: observations},
abstract = {GalSim is a collaborative, open-source project aimed at providing an image simulation tool of enduring benefit to the astronomical community. It provides a software library for generating images of astronomical objects such as stars and galaxies in a variety of ways, efficiently handling image transformations and operations such as convolution and rendering at high precision. We describe the GalSim software and its capabilities, including necessary theoretical background. We demonstrate that the performance of GalSim meets the stringent requirements of high precision image analysis applications such as weak gravitational lensing, for current datasets and for the Stage IV dark energy surveys of the Large Synoptic Survey Telescope, ESA’s Euclid mission, and NASA’s WFIRST-AFTA mission. The GalSim project repository is public and includes the full code history, all open and closed issues, installation instructions, documentation, and wiki pages (including a Frequently Asked Questions section). The GalSim repository can be found at https://github.com/GalSim-developers/GalSim.}
}

@article{Lanusse2021,
    author = {Lanusse, François and Mandelbaum, Rachel and Ravanbakhsh, Siamak and Li, Chun-Liang and Freeman, Peter and Póczos, Barnabás},
    title = "{Deep generative models for galaxy image simulations}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {504},
    number = {4},
    pages = {5543-5555},
    year = {2021},
    month = {05},
    abstract = "{Image simulations are essential tools for preparing and validating the analysis of current and future wide-field optical surveys. However, the galaxy models used as the basis for these simulations are typically limited to simple parametric light profiles, or use a fairly limited amount of available space-based data. In this work, we propose a methodology based on deep generative models to create complex models of galaxy morphologies that may meet the image simulation needs of upcoming surveys. We address the technical challenges associated with learning this morphology model from noisy and point spread function (PSF)-convolved images by building a hybrid Deep Learning/physical Bayesian hierarchical model for observed images, explicitly accounting for the PSF and noise properties. The generative model is further made conditional on physical galaxy parameters, to allow for sampling new light profiles from specific galaxy populations. We demonstrate our ability to train and sample from such a model on galaxy postage stamps from the HST/ACS COSMOS survey, and validate the quality of the model using a range of second- and higher order morphology statistics. Using this set of statistics, we demonstrate significantly more realistic morphologies using these deep generative models compared to conventional parametric models. To help make these generative models practical tools for the community, we introduce galsim-hub, a community-driven repository of generative models, and a framework for incorporating generative models within the galsim image simulation software.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stab1214},
    url = {https://doi.org/10.1093/mnras/stab1214},
    eprint = {https://academic.oup.com/mnras/article-pdf/504/4/5543/38036124/stab1214.pdf},
}

@INPROCEEDINGS{Zhang2017,
  author={Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={5908-5916},
  keywords={Gallium nitride;Training;Generators;Manifolds;Image resolution;Shape;Gaussian distribution},
  doi={10.1109/ICCV.2017.629}
}

@article{Fussell2019,
    author = {Fussell, Levi and Moews, Ben},
    title = "{Forging new worlds: high-resolution synthetic galaxies with chained generative adversarial networks}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {485},
    number = {3},
    pages = {3203-3214},
    year = {2019},
    month = {03},
    abstract = "{Astronomy of the 21st century increasingly finds itself with extreme quantities of data. This growth in data is ripe for modern technologies such as deep image processing, which has the potential to allow astronomers to automatically identify, classify, segment, and deblend various astronomical objects. In this paper, we explore the use of chained generative adversarial networks (GANs), a class of generative models that learn mappings from latent spaces to data distributions by modelling the joint distribution of the data, to produce physically realistic galaxy images as one use case of such models. In cosmology, such data sets can aid in the calibration of shape measurements for weak lensing by augmenting data with synthetic images. By measuring the distributions of multiple physical properties, we show that images generated with our approach closely follow the distributions of real galaxies, further establishing state-of-the-art GAN architectures as a valuable tool for modern-day astronomy.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stz602},
    url = {https://doi.org/10.1093/mnras/stz602},
    eprint = {https://academic.oup.com/mnras/article-pdf/485/3/3203/28221657/stz602.pdf},
}




@article{Hemmati_2022,
doi = {10.3847/1538-4357/aca1b8},
url = {https://dx.doi.org/10.3847/1538-4357/aca1b8},
year = {2022},
month = {dec},
publisher = {The American Astronomical Society},
volume = {941},
number = {2},
pages = {141},
author = {Shoubaneh Hemmati and Eric Huff and Hooshang Nayyeri and Agnès Ferté and Peter Melchior and Bahram Mobasher and Jason Rhodes and Abtin Shahidi and Harry Teplitz},
title = {Deblending Galaxies with Generative Adversarial Networks},
journal = {The Astrophysical Journal},
abstract = {Deep generative models including generative adversarial networks (GANs) are powerful unsupervised tools in learning the distributions of data sets. Building a simple GAN architecture in PyTorch and training on the CANDELS data set, we generate galaxy images with the Hubble Space Telescope (HST) resolution starting from a noise vector. We proceed by modifying the GAN architecture to improve Subaru Hyper Suprime-Cam (HSC) ground-based images by increasing their resolution to the HST resolution. We use the super-resolution GAN on a large sample of blended galaxies, which we create using CANDELS cutouts. In our simulated blend sample, ∼20% would unrecognizably be blended even in the HST-resolution cutouts. In the HSC-like cutouts this fraction rises to ∼90%. With our modified GAN we can lower this value to ∼50%. We quantify the blending fraction in the high, low, and GAN resolutions over the whole manifold of angular separation, flux ratios, sizes, and redshift difference between the two blended objects. The two peaks found by the GAN deblender result in improvement by a factor of 10 in the photometry measurement of the blended objects. Modifying the architecture of the GAN, we also train a multiwavelength GAN with HST cutouts in seven optical + near-infrared bands. This multiwavelength GAN improves the fraction of detected blends by another ∼10% compared to the single-band GAN. This is most beneficial to the current and future precision cosmology experiments (e.g., LSST, SPHEREx, Euclid, Roman), specifically those relying on weak gravitational lensing, where blending is a major source of systematic error.}
}


@ARTICLE{Yu2015,
       author = {{Yu}, Fisher and {Seff}, Ari and {Zhang}, Yinda and {Song}, Shuran and {Funkhouser}, Thomas and {Xiao}, Jianxiong},
        title = "{LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2015,
        month = jun,
          eid = {arXiv:1506.03365},
        pages = {arXiv:1506.03365},
          doi = {10.48550/arXiv.1506.03365},
archivePrefix = {arXiv},
       eprint = {1506.03365},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150603365Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@INPROCEEDINGS{Liu2015,
  author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Deep Learning Face Attributes in the Wild}, 
  year={2015},
  volume={},
  number={},
  pages={3730-3738},
  keywords={Face;Feature extraction;Training;Face recognition;Machine learning;Support vector machines;Image recognition},
  doi={10.1109/ICCV.2015.425}
}


@inproceedings{KarrasALL18,
  title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  author = {Tero Karras and Timo Aila and Samuli Laine and Jaakko Lehtinen},
  year = {2018},
  url = {https://openreview.net/forum?id=Hk99zCeAb},
  researchr = {https://researchr.org/publication/KarrasALL18},
  cites = {0},
  citedby = {0},
  booktitle = {6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
}

@inproceedings{Kingma2018,
 author = {Kingma, Durk P and Dhariwal, Prafulla},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
 volume = {31},
 year = {2018},
 series = {NIPS '18}

}


@INPROCEEDINGS{Rombach2022,
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={10674-10685},
  keywords={Training;Visualization;Image synthesis;Computational modeling;Noise reduction;Superresolution;Process control;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01042}
}



@ARTICLE{ramesh2022,
       author = {{Ramesh}, Aditya and {Dhariwal}, Prafulla and {Nichol}, Alex and {Chu}, Casey and {Chen}, Mark},
        title = "{Hierarchical Text-Conditional Image Generation with CLIP Latents}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2022,
        month = apr,
          eid = {arXiv:2204.06125},
        pages = {arXiv:2204.06125},
          doi = {10.48550/arXiv.2204.06125},
archivePrefix = {arXiv},
       eprint = {2204.06125},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220406125R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{Oppenlaender2022,
author = {Oppenlaender, Jonas},
title = {The Creativity of Text-to-Image Generation},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569219.3569352},
doi = {10.1145/3569219.3569352},
booktitle = {Proceedings of the 25th International Academic Mindtrek Conference},
pages = {192–202},
numpages = {11},
keywords = {text-to-image generation, text-guided image synthesis, prompt engineering, generative art, creativity, Midjourney, AI art},
location = {Tampere, Finland},
series = {Academic Mindtrek '22}
}


@ARTICLE{Lempereur2024,
       author = {{Lempereur}, Etienne and {Mallat}, St{\'e}phane},
        title = "{Hierarchic Flows to Estimate and Sample High-dimensional Probabilities}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Physics - Fluid Dynamics},
         year = 2024,
        month = may,
          eid = {arXiv:2405.03468},
        pages = {arXiv:2405.03468},
          doi = {10.48550/arXiv.2405.03468},
archivePrefix = {arXiv},
       eprint = {2405.03468},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240503468L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}






@inproceedings{kadkhodaie2024generalization,
title={Generalization in diffusion models arises from geometry-adaptive harmonic representations},
author={Zahra Kadkhodaie and Florentin Guth and Eero P Simoncelli and St{\'e}phane Mallat},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ANvmVS2Yr0}
}

@article{Cheng:2023imk,
    author = {Cheng, Sihao and Morel, Rudy and Allys, Erwan and Ménard, Brice and Mallat, Stéphane},
    title = "{Scattering spectra models for physics}",
    journal = {PNAS Nexus},
    volume = {3},
    number = {4},
    pages = {pgae103},
    year = {2024},
    month = {03},
    abstract = "{Physicists routinely need probabilistic models for a number of tasks such as parameter inference or the generation of new realizations of a field. Establishing such models for highly non-Gaussian fields is a challenge, especially when the number of samples is limited. In this paper, we introduce scattering spectra models for stationary fields and we show that they provide accurate and robust statistical descriptions of a wide range of fields encountered in physics. These models are based on covariances of scattering coefficients, i.e. wavelet decomposition of a field coupled with a pointwise modulus. After introducing useful dimension reductions taking advantage of the regularity of a field under rotation and scaling, we validate these models on various multiscale physical fields and demonstrate that they reproduce standard statistics, including spatial moments up to fourth order. The scattering spectra provide us with a low-dimensional structured representation that captures key properties encountered in a wide range of physical fields. These generic models can be used for data exploration, classification, parameter inference, symmetry detection, and component separation.}",
    issn = {2752-6542},
    doi = {10.1093/pnasnexus/pgae103},
}


@article{Lee2019, 
doi = {10.21105/joss.01237}, 
year = {2019}, 
publisher = {The Open Journal}, 
volume = {4}, 
number = {36}, 
pages = {1237}, 
author = {Gregory R. Lee and Ralf Gommers and Filip Waselewski and Kai Wohlfahrt and Aaron O'Leary}, 
title = {PyWavelets: A Python package for wavelet analysis}, 
journal = {Journal of Open Source Software} 
}
 

@inproceedings{Ho2020,
author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
title = {Denoising diffusion probabilistic models},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {574},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{Bengio2013,
author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
title = {Generalized denoising auto-encoders as generative models},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
pages = {899–907},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@InProceedings{Sohl-Dickstein2015,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

 
@article{hyvarinen2005a,
  author  = {Aapo Hyv{{\"a}}rinen},
  title   = {Estimation of Non-Normalized Statistical Models by Score Matching},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {24},
  pages   = {695--709},
}
@InProceedings{DenoisingReview2023,
author="Rama Lakshmi, Gali
and Divya, G.
and Bhavya, D.
and Sai Jahnavi, Ch.
and Akila, B.",
editor="Bindhu, V.
and Tavares, Jo{\~a}o Manuel R. S.
and Vuppalapati, Chandrasekar",
title="A Review on Image Denoising Algorithms for Various Applications",
booktitle="Proceedings of Fourth International Conference on Communication, Computing and Electronics Systems ",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="839--847",
abstract="Image is well-known word in various fields like medical, engineering, and arts. A literature review is conducted on image qualities, and the need to improve the quality of image for various applications are identified. Numerous noises are added to an image both internally and externally in different stages which results in the poor quality of image. At few points, the noise in the image leads to loss of critical information and creates a lot of damage in that area. Purpose of this review is to gain knowledge about the image denoising and give details about denoising filter algorithms used for noise removal from images. Various techniques exist to overcome noise in any image. Multiple filters and combinations of filters have been designed to remove noise, and those are summarised in this paper for better understanding.",
isbn="978-981-19-7753-4"
}


@article{Wang2020,
author = {Wang, Hua and Fan, Linwei and Guo, Qiang and Zhang, Caiming},
year = {2020},
month = {01},
pages = {461-480},
title = {A review of image denoising methods},
volume = {20},
journal = {Communications in Information and Systems},
doi = {10.4310/CIS.2020.v20.n4.a4}
}

@misc{Mathematica,
  author = {{Wolfram Research, Inc.}},
  title = {Mathematica 13.1},
  url = {https://www.wolfram.com},
  year = {2022},
}
@misc{MATLAB,
author = {The MathWorks, Inc.},
year = {2024},
title = {MATLAB version: 24.1.0.2537033 (R2024a)},
publisher = {The MathWorks Inc.},
address = {Natick, Massachusetts, United States},
url = {https://www.mathworks.com}
}

@inproceedings{AlexNet2012,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 volume = {25},
 year = {2012}
}




@article{Jaynes1957,
  title = {Information Theory and Statistical Mechanics},
  author = {Jaynes, E. T.},
  journal = {Phys. Rev.},
  volume = {106},
  issue = {4},
  pages = {620--630},
  numpages = {0},
  year = {1957},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.106.620},
}

@article{shannon1948mathematical,
  added-at = {2013-02-27T08:21:47.000+0100},
  author = {Shannon, Claude E.},
  biburl = {https://www.bibsonomy.org/bibtex/2b6bc42c140f0147cd6a1781d75fcb897/jaeschke},
  interhash = {754130207906fcec16a53d330eeff348},
  intrahash = {b6bc42c140f0147cd6a1781d75fcb897},
  journal = {The Bell System Technical Journal},
  keywords = {communication ddm entropy information mk3.3 shannon toread},
  month = {July, October},
  pages = {379--423, 623--656},
  timestamp = {2021-10-15T09:15:42.000+0200},
  title = {A Mathematical Theory of Communication},
  volume = 27,
  year = 1948
}

@article{Guth2022b,
  title={Wavelet score-based generative modeling},
  author={Guth, Florentin and Coste, Simon and De Bortoli, Valentin and Mallat, Stephane},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={478--491},
  year={2022}
}



@article{LinYang2023,
author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626235},
doi = {10.1145/3626235},
abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github:},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {105},
numpages = {39},
keywords = {stochastic differential equations, score-based generative models, diffusion models, Generative models}
}
@ARTICLE{Chang2023,
       author = {{Chang}, Ziyi and {Koulieris}, George Alex and {Shum}, Hubert P.~H.},
        title = "{On the Design Fundamentals of Diffusion Models: A Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
         year = 2023,
        month = jun,
          eid = {arXiv:2306.04542},
        pages = {arXiv:2306.04542},
          doi = {10.48550/arXiv.2306.04542},
archivePrefix = {arXiv},
       eprint = {2306.04542},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230604542C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{Guth2022a,
       author = {{Guth}, Florentin and {Coste}, Simon and {De Bortoli}, Valentin and {Mallat}, Stephane},
        title = "{Wavelet Score-Based Generative Modeling}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
         year = 2022,
        month = aug,
          eid = {arXiv:2208.05003},
        pages = {arXiv:2208.05003},
          doi = {10.48550/arXiv.2208.05003},
archivePrefix = {arXiv},
       eprint = {2208.05003},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220805003G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Uhlenbeck1930,
  title = {On the Theory of the Brownian Motion},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.},
  journal = {Phys. Rev.},
  volume = {36},
  issue = {5},
  pages = {823--841},
  numpages = {0},
  year = {1930},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.36.823},
}

@article{robbins1964empirical,
  title={An empirical Bayes approach to statistics},
  author={Robbins, Herbert Ellis},
  journal={Matematika},
  volume={8},
  number={2},
  pages={133--140},
  year={1964}
}

@inproceedings{herbert1956empirical,
  title={An empirical Bayes approach to statistics},
  author={Herbert, Robbins},
  booktitle={Proceedings of the third berkeley symposium on mathematical statistics and probability},
  volume={1},
  pages={157--163},
  year={1956}
}

@article{miyasawa1961empirical,
  title={An empirical Bayes estimator of the mean of a normal population},
  author={Miyasawa, Koichi and others},
  journal={Bull. Inst. Internat. Statist},
  volume={38},
  number={181-188},
  pages={1--2},
  year={1961}
}

@article{Raphan2011,
author = {Raphan, Martin and Simoncelli, Eero P.},
title = {Least squares estimation without priors or supervision},
year = {2011},
issue_date = {February 2011},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {23},
number = {2},
issn = {0899-7667},
doi = {10.1162/NECO_a_00076},
abstract = {Selection of an optimal estimator typically relies on either supervised training samples (pairs of measurements and their associated true values) or a prior probability model for the true values. Here, we consider the problem of obtaining a least squares estimator given a measurement process with known statistics (i.e., a likelihood function) and a set of unsupervised measurements, each arising from a corresponding true value drawn randomly from an unknown distribution. We develop a general expression for a nonparametric empirical Bayes least squares (NEBLS) estimator, which expresses the optimal least squares estimator in terms of the measurement density, with no explicit reference to the unknown (prior) density. We study the conditions under which such estimators exist and derive specific forms for a variety of different measurement processes. We further show that each of these NEBLS estimators may be used to express the mean squared estimation error as an expectation over the measurement density alone, thus generalizing Stein's unbiased risk estimator (SURE), which provides such an expression for the additive gaussian noise case. This error expression may then be optimized over noisy measurement samples, in the absence of supervised training data, yielding a generalized SURE-optimized parametric least squares (SURE2PLS) estimator. In the special case of a linear parameterization (i.e., a sum of nonlinear kernel functions), the objective function is quadratic, and we derive an incremental form for learning this estimator from data. We also show that combining the NEBLS form with its corresponding generalized SURE expression produces a generalization of the score-matching procedure for parametric density estimation. Finally, we have implemented several examples of such estimators, and we show that their performance is comparable to their optimal Bayesian or supervised regression counterparts for moderate to large amounts of data.},
journal = {Neural Comput.},
month = {feb},
pages = {374–420},
numpages = {47}
}
@book{Wiener1949,
    author = {Wiener, Norbert},
    title = "{Extrapolation, Interpolation, and Smoothing of Stationary Time Series: With Engineering Applications}",
    publisher = {The MIT Press},
    year = {1949},
    month = {08},
    abstract = "{A book thatbecame the basis for modern communication theory, by a scientist considered one of the founders of the field of artifical intelligence.Some predict that Norbert Wiener will be remembered for his Extrapolation long after Cybernetics is forgotten. Indeed, few computer science students would know today what cybernetics is all about, while every communication student knows what Wiener's filter is. The original work was circulated as a classified memorandum in 1942, because it was connected with sensitive wartime efforts to improve radar communication. This book became the basis for modern communication theory, by a scientist considered one of the founders of the field of artifical intelligence. Combining ideas from statistics and time-series analysis, Wiener used Gauss's method of shaping the characteristic of a detector to allow for the maximal recognition of signals in the presence of noise. This method came to be known as the "Wiener filter."}",
    isbn = {9780262257190},
    doi = {10.7551/mitpress/2946.001.0001},
}




@article{MallatPeyre2008,
  TITLE = {{Orthogonal Bandlet Bases for Geometric Images Approximation}},
  AUTHOR = {Mallat, St{\'e}phane and Peyr{\'e}, Gabriel},
  URL = {https://hal.science/hal-00359740},
  JOURNAL = {{Communications on Pure and Applied Mathematics}},
  PUBLISHER = {{Wiley}},
  VOLUME = {61},
  NUMBER = {9},
  PAGES = {1173-1212},
  YEAR = {2008},
  MONTH = Sep,
  KEYWORDS = {Bandlets ; image compression ; orthogonal bandlets ; geometric images},
  HAL_ID = {hal-00359740},
  HAL_VERSION = {v1},
}

@book{korostelev1993minimax,
  title={Minimax Theory of Image Reconstruction},
  author={Korostelev, A.P. and Tsybakov, A.B.},
  isbn={9783540940289},
  lccn={93018028},
  series={Lecture notes in statistics},
  year={1993},
  publisher={Springer-Verlag}
}

@article{Ferroukhi2019,
author = {Ferroukhi, Merzak and Ouahabi, A. and Attari, Mokhtar and taleb-ahmed, Abdelmalik},
year = {2019},
month = {01},
pages = {88},
title = {Medical Video Coding Based on 2nd-Generation Wavelets: Performance Evaluation},
volume = {8},
journal = {Electronics},
doi = {10.3390/electronics8010088}
}
@article{ZHANG2019a,
title = {High noise astronomical image denoising via 2G-bandelet denoising compressed sensing},
journal = {Optik},
volume = {184},
pages = {377-388},
year = {2019},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2019.04.029},
author = {Jie Zhang and Huanlong Zhang and Xiaoping Shi and Shengtao Geng},
keywords = {Cosmic noise, Compressed sensing, Astronomical image denoising, 2G-bandelet, Iterative bandelet thresholding, GSTV-SC method},
abstract = {In deep space exploration, high resolution astronomical image captured is often contaminated by various cosmic noise signals during its shooting and long distance transmission, which has brought inconvenience to astronomical image analysis. The famous compressed sensing (CS) proposed by Candes et al. can successfully solve the problem of high resolution astronomical image compression and low noise reconstruction. In this paper, we further concern how to reconstruct a high quality image from a high resolution and high noise astronomical image. A 2G-bandelet denoising compressed sensing (BDCS) is first proposed based on the advantage of CS in image denoising and the superior ability of 2G-bandelet in sparse representation of astronomical images, then iterative bandelet thresholding (IBT-BTCS) algorithm based on BDCS is proposed for high resolution and high noise astronomical image reconstruction. Firstly, an iterative bandelet thresholding method is designed to obtain optimal approximation of original image; Secondly, to further improve the reconstructed image quality, group sparse total variation with stepsize constraints (GSTV-SC) method is proposed to adjust the reconstructed astronomical image in each iteration. The simulation results show that the proposed algorithm can quickly reconstruct a high quality astronomical image only using a few observations, preserve more astronomical image details and effectively solve high noise astronomical image denoising problem.}
}

@misc{kong2023comparison,
      title={A Comparison of Image Denoising Methods}, 
      author={Zhaoming Kong and Fangxi Deng and Haomin Zhuang and Jun Yu and Lifang He and Xiaowei Yang},
      year={2023},
      eprint={2304.08990},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@article{Vergara2008,
author = {Vergara, Osslan and Ochoa, Humberto and Sànchez, Vianey},
year = {2008},
month = {10},
pages = {207-212},
title = {A Comparison of the Bandelet, Wavelet and Contourlet Transforms for Image Denoising},
volume = {0},
isbn = {978-0-7695-3441-1},
journal = {Mexican International Conference on Artificial Intelligence},
doi = {10.1109/MICAI.2008.63}
}

@article{Dossal2011,
  TITLE = {{Bandlet Image Estimation with Model Selection}},
  AUTHOR = {Dossal, Charles H and Le Pennec, Erwan and Mallat, St{\'e}phane},
  JOURNAL = {{Signal Processing}},
  PUBLISHER = {{Elsevier}},
  VOLUME = {91},
  NUMBER = {12},
  PAGES = {2743-2753},
  YEAR = {2011},
  MONTH = Jan,
  DOI = {10.1016/j.sigpro.2011.01.013},
  PDF = {https://hal.science/hal-00321965v2/file/BandletEstim.pdf},
  HAL_ID = {hal-00321965},
  HAL_VERSION = {v2},
}


@InProceedings{Pennec2007,
  author =	 {Le Pennec, E. and Dossal, Ch. and Peyré, G. and
                  Mallat, S. },
  title =	 {Débruitage géométrique d'image dans des bases
                  orthonormées de bandelettes},
  booktitle =	 {GRETSI 07},
  year =	 2007,
  address =	 {Troyes},
  x-international-audience ={no},
  x-proceedings ={yes},
  x-invited-conference ={no},
  keywords =	 {Actes, Bandlets, ThemeBandlet},
  pubtype =	 {Actes de conférence nationale},
  subject =	 {Bandlets},
  preprint =	 {Bandlets/2007-GRETSI-LPDPM.pdf},
  reprint =	 {Bandlets/2007-GRETSI-LPDPM.pdf},
  url_PDF =
                  {http://lepennec.perso.math.cnrs.fr/Reprint/Bandlets/2007-GRETSI-LPDPM.pdf},
  reprintok =	 1,
  lang =	 {Francais},
  hal =		 {hal-00365965},
}

@inproceedings{Mclaughlin2015,
author = {Mclaughlin, Michael and Grieggs, Samuel and Ezekiel, Soundararajan and Ferris, Michael and Blasch, Erik and Alford, Mark and Cornacchia, Maria and Bubalo, Adnan},
year = {2015},
month = {06},
pages = {},
title = {Bandelet Denoising in Image Processing},
doi = {10.1109/NAECON.2015.7443035}
}

@article{Pennec2005b,
author = {Le Pennec, Erwan and Mallat, Stéphane},
year = {2005},
month = {01},
pages = {},
title = {Bandelet Image Approximation and Compression},
volume = {4},
journal = {Multiscale Modeling \& Simulation},
doi = {10.1137/040619454}
}
@article{Pennec2005a,
author = {Le Pennec, Erwan and Mallat, Stéphane},
year = {2005},
month = {05},
pages = {423-38},
title = {Sparse Geometric Image Representations with Bandelets},
volume = {14},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
doi = {10.1109/TIP.2005.843753}
}

@article{MallatPeyre2007,
author = {Mallat, Stéphane and Peyré, Gabriel},
year = {2007},
month = {07},
pages = {},
title = {A Review of Bandlet Methods for Geometrical Image Representation},
volume = {44},
journal = {Numerical Algorithms},
doi = {10.1007/s11075-007-9092-4}
}


@inproceedings{liu2015faceattributes,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}





@article{Eslahi2016,
author = {Eslahi, Nasser and Aghagolzadeh, Ali},
title = {Compressive Sensing Image Restoration Using Adaptive Curvelet Thresholding and Nonlocal Sparse Regularization},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {1057-7149},
doi = {10.1109/TIP.2016.2562563},
abstract = {Compressive sensing (CS) is a recently emerging technique and an extensively studied problem in signal and image processing, which suggests a new framework for the simultaneous sampling and compression of sparse or compressible signals at a rate significantly below the Nyquist rate. Maybe, designing an effective regularization term reflecting the image sparse prior information plays a critical role in CS image restoration. Recently, both local smoothness and nonlocal self-similarity have led to superior sparsity prior for CS image restoration. In this paper, first, an adaptive curvelet thresholding criterion is developed, trying to adaptively remove the perturbations appeared in recovered images during CS recovery process, imposing sparsity. Furthermore, a new sparsity measure called joint adaptive sparsity regularization (JASR) is established, which enforces both local sparsity and nonlocal 3-D sparsity in transform domain, simultaneously. Then, a novel technique for high-fidelity CS image recovery via JASR is proposed—CS-JASR. To efficiently solve the proposed corresponding optimization problem, we employ the split Bregman iterations. Extensive experimental results are reported to attest the adequacy and effectiveness of the proposed method comparing with the current state-of-the-art methods in CS image restoration.},
journal = {Trans. Img. Proc.},
month = {jul},
pages = {3126–3140},
numpages = {15}
}


@article{Candes2006a,
author = {Candès, Emmanuel and Demanet, Laurent and Donoho, David and Ying, Lexing},
year = {2006},
month = {09},
pages = {},
title = {Fast Discrete Curvelet Transforms},
volume = {5},
journal = {SIAM Journal on Multiscale Modeling and Simulation},
doi = {10.1137/05064182X}
}

@article{Hergt2017,
   title={Searching for cosmic strings in CMB anisotropy maps using wavelets and curvelets},
   volume={2017},
   ISSN={1475-7516},
   DOI={10.1088/1475-7516/2017/06/004},
   number={06},
   journal={Journal of Cosmology and Astroparticle Physics},
   publisher={IOP Publishing},
   author={Hergt, Lukas and Amara, Adam and Brandenberger, Robert and Kacprzak, Tomasz and Réfrégier, Alexandre},
   year={2017},
   month=jun, pages={004–004} 
}

@article{Woiselle2010,
author = {Woiselle, A. and Starck, Jean-Luc and Fadili, Jalal},
year = {2010},
month = {03},
pages = {171-188},
title = {3D Curvelet transforms and Astronomical Data Restoration},
volume = {28},
journal = {Applied and Computational Harmonic Analysis},
doi = {10.1016/j.acha.2009.12.003}
}

@article{Starck2003a,
	author = {{Starck, J. L.} and {Donoho, D. L.} and {Candès, E. J.}},
	title = {Astronomical image representation by the curvelet transform},
	DOI= "10.1051/0004-6361:20021571",
	journal = {\aap},
	year = 2003,
	volume = 398,
	number = 2,
	pages = "785-800"
}

@ARTICLE{Starck2002a,
  author={Jean-Luc Starck and Candes, E.J. and Donoho, D.L.},
  journal={IEEE Transactions on Image Processing}, 
  title={The curvelet transform for image denoising}, 
  year={2002},
  volume={11},
  number={6},
  pages={670-684},
  keywords={Image denoising;Wavelet transforms;Image reconstruction;Interpolation;Wavelet domain;Filter bank;Stability;Computational complexity;Fourier transforms;Sampling methods},
  doi={10.1109/TIP.2002.1014998}
}


@article{Candes2000a,
author = {Emmanuel J. Candes and David L. Donoho},
year = {2000},
month = {11},
pages = {},
title = {Curvelets, Multiresolution Representation, and Scaling Laws},
volume = {4119},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.408568}
}

@article{Donoho2000a,
author = {Donoho, David L.},
title = {Orthonormal Ridgelets and Linear Singularities},
journal = {SIAM Journal on Mathematical Analysis},
volume = {31},
number = {5},
pages = {1062-1099},
year = {2000},
doi = {10.1137/S0036141098344403},
}



@article{Candes2002,
author = {Candès, Emmanuel and Guo, Franck},
year = {2002},
month = {11},
pages = {1519-1543},
title = {New multiscale transforms, minimum total variation synthesis: Applications to edge-preserving image reconstruction},
volume = {82},
journal = {Signal Processing},
doi = {10.1016/S0165-1684(02)00300-6}
}

@article{Boshra2014,
    title   = {{An Analysis and Improvement of the BLS-GSM Denoising Method}},
    author  = {Rajaei, Boshra},
    journal = {{Image Processing On Line}},
    volume  = {4},
    pages   = {44--70},
    year    = {2014},
    note    = {\url{https://doi.org/10.5201/ipol.2014.86}}
}

@inproceedings{Simoncelli1995b,
author = {Simoncelli, E. P. and Freeman, W. T.},
title = {The steerable pyramid: a flexible architecture for multi-scale derivative computation},
year = {1995},
isbn = {0818673109},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We describe an architecture for efficient and accurate linear decomposition of an image into scale and orientation subbands. The basis functions of this decomposition are directional derivative operators of any desired order. We describe the construction and implementation of the transform.},
booktitle = {Proceedings of the 1995 International Conference on Image Processing (Vol. 3)-Volume 3 - Volume 3},
pages = {3444},
keywords = {transform, steerable pyramid, scale subbands, orientation subbands, multiscale derivative computation, linear image decomposition, image processing, flexible architecture, filters, filtering theory, directional derivative operators, building, basis functions, Fourier transforms, Fourier domain},
series = {ICIP '95}
}

@article{Sze2017EfficientPO,
  title={Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  author={Vivienne Sze and Yu-hsin Chen and Tien-Ju Yang and Joel S. Emer},
  journal={Proceedings of the IEEE},
  year={2017},
  volume={105},
  pages={2295-2329},
}
@ARTICLE{BrunaMallat2013,
  author={Bruna, Joan and Mallat, Stephane},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Invariant Scattering Convolution Networks}, 
  year={2013},
  volume={35},
  number={8},
  pages={1872-1886},
  keywords={Scattering;Convolution;Fourier transforms;Wavelet coefficients;Computer architecture;Classification;convolution networks;deformations;invariants;wavelets},
  doi={10.1109/TPAMI.2012.230}}

@ARTICLE{2012arXiv1203.1513B,
       author = {{Bruna}, Joan and {Mallat}, St{\'e}phane},
        title = "{Invariant Scattering Convolution Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2012,
        month = mar,
          eid = {arXiv:1203.1513},
        pages = {arXiv:1203.1513},
          doi = {10.48550/arXiv.1203.1513},
archivePrefix = {arXiv},
       eprint = {1203.1513},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012arXiv1203.1513B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Cheng2020,
   title={A new approach to observational cosmology using the scattering transform},
   volume={499},
   ISSN={1365-2966},
   DOI={10.1093/mnras/staa3165},
   number={4},
   journal={Monthly Notices of the Royal Astronomical Society},
   publisher={Oxford University Press (OUP)},
   author={Cheng, Sihao and Ting, Yuan-Sen and Ménard, Brice and Bruna, Joan},
   year={2020},
   month=oct, pages={5902–5914} }

@inproceedings{Zarka2021,
  TITLE = {{Separation and Concentration in Deep Networks}},
  AUTHOR = {Zarka, John and Guth, Florentin and Mallat, St{\'e}phane},
  BOOKTITLE = {{ICLR 2021 - 9th International Conference on Learning Representations}},
  ADDRESS = {Vienna / Virtual, Austria},
  YEAR = {2021},
  MONTH = May,
  HAL_ID = {hal-03169904},
  HAL_VERSION = {v1},
}

@inproceedings{Zarka2019,
title={Deep Network Classification by Scattering and Homotopy Dictionary Learning},
author={John Zarka and Louis Thiry and Tomas Angles and Stephane Mallat},
booktitle={International Conference on Learning Representations},
year={2020},
}



@ARTICLE{2013arXiv1312.6114K,
       author = {{Kingma}, Diederik P and {Welling}, Max},
        title = "{Auto-Encoding Variational Bayes}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2013,
        month = dec,
       eprint = {1312.6114},
 primaryClass = {stat.ML},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{2017ITIP...26.3142Z,
       author = {{Zhang}, Kai and {Zuo}, Wangmeng and {Chen}, Yunjin and {Meng}, Deyu and {Zhang}, Lei},
        title = "{Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising}",
      journal = {IEEE Transactions on Image Processing},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2017,
        month = jul,
       volume = {26},
       number = {7},
        pages = {3142-3155},
          doi = {10.1109/TIP.2017.2662206},
 primaryClass = {cs.CV},
}


@inproceedings{Tang2012,
author = {Tang, Yichuan and Salakhutdinov, R. and Hinton, G.},
year = {2012},
month = {06},
pages = {2264-2271},
title = {Robust Boltzmann Machines for recognition and denoising},
isbn = {978-1-4673-1226-4},
journal = {Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247936}
}


@InProceedings{ronneberger2015u,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}


@inproceedings{song2021scorebased,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
  booktitle={International Conference on Learning Representations},
  year={2021},
}

@article{2018CampagneMallatNotesold,
	author = { Campagne, Jean-Eric},
	title = "Notes et commentaires au sujet des conférences de S. Mallat du Collège de France",
	year = 2018,
	url = {https://github.com/jecampagne/cours_mallat_cdf/blob/main/Notes/Resume-2018.pdf},
}

@unpublished{2018CampagneMallatNotes,
  TITLE = {{Notes et commentaires au sujet des conf{\'e}rences de S. Mallat du Coll{\`e}ge de France (2018)}},
  AUTHOR = {Campagne, Jean-Eric},
  URL = {https://hal.science/hal-04548691},
  TYPE = {Master},
  ADDRESS = {https://www.college-de-france.fr/fr/agenda/cours/apprentissage-face-la-malediction-de-la-grande-dimension, France},
  PAGES = {109},
  INSTITUTION = {{Coll{\`e}ge de France}},
  YEAR = {2018},
  MONTH = Jan,
  KEYWORDS = {Apprentissage automatique Machine Learning ; R{\'e}seau de neurone convolutif ; SVM - Support Vector Machines},
  PDF = {https://hal.science/hal-04548691/file/Resume-2018.pdf},
  HAL_ID = {hal-04548691},
  HAL_VERSION = {v1},
}


@article{2019CampagneMallatNotesold,
	author = { Campagne, Jean-Eric},
	title = "Notes et commentaires au sujet des conférences de S. Mallat du Collège de France",
	year = 2019,
	url = {https://github.com/jecampagne/cours_mallat_cdf/blob/main/Notes/Resume-2019.pdf},
}

@unpublished{2019CampagneMallatNotes,
  TITLE = {{Notes et commentaires au sujet des conf{\'e}rences de S. Mallat du Coll{\`e}ge de France (2019)}},
  AUTHOR = {Campagne, Jean-Eric},
  URL = {https://hal.science/hal-04549194},
  TYPE = {Doctoral},
  ADDRESS = {https://www.college-de-france.fr/fr/agenda/cours/apprentissage-par-reseaux-de-neurones-profonds, France},
  PAGES = {144},
  INSTITUTION = {{Coll{\`e}ge de France}},
  YEAR = {2019},
  MONTH = Jan,
  KEYWORDS = {Apprentissage automatique Machine Learning ; R{\'e}seau de neurone convolutif ; Descente de Gradient ; Mal{\'e}diction de la Dimension},
  PDF = {https://hal.science/hal-04549194/file/Resume-2019.pdf},
  HAL_ID = {hal-04549194},
  HAL_VERSION = {v1},
}

@article{2020CampagneMallatNotesold,
	author = { Campagne, Jean-Eric},
	title = "Notes et commentaires au sujet des conférences de S. Mallat du Collège de France",
	year = 2020,
	url = {https://github.com/jecampagne/cours_mallat_cdf/blob/main/Notes/Resume-2020.pdf},
}

@unpublished{2020CampagneMallatNotes,
  TITLE = {{Notes et commentaires au sujet des conf{\'e}rences de S. Mallat du Coll{\`e}ge de France (2020)}},
  AUTHOR = {Campagne, Jean-Eric},
  URL = {https://hal.science/hal-04549242},
  TYPE = {Master},
  ADDRESS = {https://www.college-de-france.fr/fr/agenda/cours/modeles-multi-echelles-et-reseaux-de-neurones-convolutifs, France},
  PAGES = {146},
  INSTITUTION = {{Coll{\`e}ge de France}},
  YEAR = {2020},
  MONTH = Jan,
  KEYWORDS = {Apprentissage automatique Machine Learning ; R{\'e}seau de neurone convolutif ; Transform{\'e}e de Fourier ; Transform{\'e}e en Ondelettes},
  PDF = {https://hal.science/hal-04549242/file/Resume-2020.pdf},
  HAL_ID = {hal-04549242},
  HAL_VERSION = {v1},
}


@article{2021CampagneMallatNotesold,
	author = { Campagne, Jean-Eric},
	title = "Notes et commentaires au sujet des conférences de S. Mallat du Collège de France",
	year = 2021,
	url = {https://github.com/jecampagne/cours_mallat_cdf/blob/main/Notes/Resume-2021.pdf},
}
@unpublished{2021CampagneMallatNotes,
  TITLE = {{Notes et commentaires au sujet des conf{\'e}rences de S. Mallat du Coll{\`e}ge de France (2021)}},
  AUTHOR = {Campagne, Jean-Eric},
  URL = {https://hal.science/hal-04549376},
  TYPE = {Master},
  ADDRESS = {France},
  PAGES = {139},
  INSTITUTION = {{Coll{\`e}ge de France}},
  YEAR = {2021},
  MONTH = Jan,
  KEYWORDS = {Apprentissage automatique Machine Learning ; R{\'e}seau de neurone convolutif ; Transform{\'e}e de Fourier ; Transform{\'e}e en Ondelettes ; JPEG ; JPEG2000},
  PDF = {https://hal.science/hal-04549376/file/Resume-2021.pdf},
  HAL_ID = {hal-04549376},
  HAL_VERSION = {v1},
}

@article{2022CampagneMallatNotesold,
	author = { Campagne, Jean-Eric},
	title = "Notes et commentaires au sujet des conférences de S. Mallat du Collège de France",
	year = 2022,
	url = {https://github.com/jecampagne/cours_mallat_cdf/blob/main/Notes/Resume-2022.pdf},
}

@unpublished{2022CampagneMallatNotes,
  TITLE = {{Notes et commentaires au sujet des conf{\'e}rences de S. Mallat du Coll{\`e}ge de France (2022)}},
  AUTHOR = {Campagne, Jean-Eric},
  URL = {https://hal.science/hal-04549438},
  TYPE = {Master},
  ADDRESS = {https://www.college-de-france.fr/fr/agenda/cours/information-et-complexite, France},
  PAGES = {134},
  INSTITUTION = {{Coll{\`e}ge de France}},
  YEAR = {2022},
  MONTH = Jan,
  KEYWORDS = {Apprentissage automatique Machine Learning ; R{\'e}seau de neurone convolutif ; Information de Fisher ; Entropie de Shannon ; JPEG ; JPEG2000 ; MPEG ; Codage entropique ; Inf{\'e}rence},
  PDF = {https://hal.science/hal-04549438/file/Resume-2022.pdf},
  HAL_ID = {hal-04549438},
  HAL_VERSION = {v1},
}


@article{2023CampagneMallatNotesold,
	author = { Campagne, Jean-Eric},
	title = "Notes et commentaires au sujet des conférences de S. Mallat du Collège de France",
	year = 2023,
	url = {https://github.com/jecampagne/cours_mallat_cdf/blob/main/Notes/Resume-2023.pdf},
}

@unpublished{2023CampagneMallatNotes,
  TITLE = {{Notes et commentaires au sujet des conf{\'e}rences de S. Mallat du Coll{\`e}ge de France (2023)}},
  AUTHOR = {Campagne, Jean-Eric},
  URL = {https://hal.science/hal-04549532},
  TYPE = {Master},
  ADDRESS = {https://www.college-de-france.fr/fr/agenda/cours/modeles-information-et-physique-statistique, France},
  PAGES = {157},
  INSTITUTION = {{Coll{\`e}ge de France}},
  YEAR = {2023},
  MONTH = Jan,
  KEYWORDS = {Apprentissage automatique Machine Learning ; R{\'e}seau de neurone convolutif ; Entropie de Shannon ; Entropie Maximale ; Chaine de Markov ; Processus ergodique ; Physique Statistique},
  PDF = {https://hal.science/hal-04549532/file/Resume-2023%20%281%29.pdf},
  HAL_ID = {hal-04549532},
  HAL_VERSION = {v1},
}

@article{2024CampagneMallatNotesold,
	author = { Campagne, Jean-Eric},
	title = "Notes et commentaires au sujet des conférences de S. Mallat du Collège de France",
	year = 2024,
	url = {https://github.com/jecampagne/cours_mallat_cdf/blob/main/Notes/Resume-2024.pdf},
}

@unpublished{2024CampagneMallatNotes,
  TITLE = {{Notes et commentaires au sujet des conf{\'e}rences de S. Mallat du Coll{\`e}ge de France (2024)}},
  AUTHOR = {Campagne, Jean-Eric},
  URL = {https://hal.science/hal-04549633},
  TYPE = {Doctoral},
  ADDRESS = {https://www.college-de-france.fr/fr/agenda/cours/apprentissage-et-generation-par-echantillonnage-aleatoire, France},
  PAGES = {169},
  INSTITUTION = {{Coll{\`e}ge de France}},
  YEAR = {2024},
  MONTH = Jan,
  KEYWORDS = {Apprentissage automatique Machine Learning ; R{\'e}seau de neurone convolutif ; Chaine de markov ; Algorithmes MCMC ; Mod{\`e}le g{\'e}n{\'e}ratif ; Information de Fisher ; Entropie de Shannon ; Equation d'Ornstein-Uhlenbeck},
  PDF = {https://hal.science/hal-04549633/file/Resume-2024.pdf},
  HAL_ID = {hal-04549633},
  HAL_VERSION = {v1},
}


@book{MallatWavelet2008,
author = {Mallat, Stéphane},
title = {A Wavelet Tour of Signal Processing, Third Edition: The Sparse Way},
year = {2008},
isbn = {0123743702},
publisher = {Academic Press, Inc.},
address = {USA},
edition = {3rd},
}
