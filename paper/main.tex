\documentclass[11pt]{amsart}
\usepackage{amsaddr}
\usepackage{mathtools}
\usepackage{aas_macros} % macro pour Bibtex
\usepackage{hyperref}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm}
\usepackage{amsmath}
% \usepackage{makecell}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{color}
\usepackage{subcaption}



\usepackage{booktabs}
\usepackage{pifont}
\usepackage{natbib,fancyhdr} %new
\usepackage{bm}

%%%
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{automata}
\usetikzlibrary{positioning}  %                 ...positioning nodes
\usetikzlibrary{arrows}   
\tikzset{stretch/.initial=1}    %                 ...customizing arrows
\newcommand\drawloop[4][]%
   {\draw[shorten <=0pt, shorten >=0pt,#1]
      ($(#2)!\pgfkeysvalueof{/tikz/stretch}!(#2.#3)$)
      let \p1=($(#2.center)!\pgfkeysvalueof{/tikz/stretch}!(#2.north)-(#2)$),
          \n1= {veclen(\x1,\y1)*sin(0.5*(#4-#3))/sin(0.5*(180-#4+#3))}
      in arc [start angle={#3-90}, end angle={#4+90}, radius=\n1]%
   }
\usepackage{tikz-cd}
\usetikzlibrary{cd}
%
%%%


%\usepackage{mathtools} % psmallmatrix

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\setlength{\heavyrulewidth}{3\lightrulewidth}
\setlength{\abovetopsep}{1ex}

\newcommand{\Esp}[0]{\ensuremath{\mathbb{E}}}
\newcommand{\DKL}[0]{\ensuremath{\mathbb{D}_{\mathsf{KL}}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\diag}{diag}

\usepackage[top=3cm, bottom=2cm, left=3cm, right=2cm]{geometry} %margins

% \usepackage{geometry}
%  \geometry{
%  a4paper,
%  left=25mm,
%  }

\newcommand{\jessa}[1]{{\color{red} #1}}
\newcommand{\field}[1]{\mathbf{#1}}
\newcommand{\nn}{\nonumber}


% My definitions:
\def\U{\mathcal U}
\def\LL{\mathcal L}
\def\P{\mathcal P}
\def\D{\mathcal D}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% My packages:
\usepackage{algorithmic}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{xcolor}
\usepackage{multicol} 
\usepackage{enumitem}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\newcommand{\bl}[1]{{\color{blue} #1}}
\newcommand{\rd}[1]{{\color{red} #1}}


% Alternative Assumption!

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}

% New environment
\newenvironment{assumptionp}[1]{
  \renewcommand\theassumptionalt{#1}
  \assumptionalt
}{\endassumptionalt}

\graphicspath{{./figures}}
\usepackage[margin=1cm]{caption}


\newenvironment{nouppercase}{%
  \let\uppercase\relax%
  \renewcommand{\uppercasenonmath}[1]{}}{}


\title{Galaxy image statistical generator: how to be confident?}
\author{Jean-Eric Campagne}
\address{Universit√© Paris-Saclay, CNRS/IN2P3, IJCLab, 91405 Orsay, France
}
\email{jean-eric.campagne AT ijclab.in2p3.fr}
\date{\today}
\calclayout  % switch off left right margin different even/odd pages.

\begin{document}
\begin{nouppercase}
\maketitle
\end{nouppercase}
%\renewcommand{\baselinestretch}{0.75}\normalsize
%\tableofcontents
%\renewcommand{\baselinestretch}{1.0}\normalsize
%
\begin{abstract}
Todo...
\\
\smallskip
\noindent \textbf{Keywords.} diffusion score based model, normalizing flows, U-Net denoiser, Glow
\end{abstract}

\section{Introduction}
\label{sec:Intro}
Image generation in Machine Learning (ML) is a challenging task especially in the context of non-ergodic processes that has made a breakthrough in quality recently thanks to the large scale statistical model architectures as for instance \texttt{DALL-E2} \citep{ramesh2022}, \texttt{Midjourney} \citep{Oppenlaender2022} and \texttt{StableDiffusion} \citep{Rombach2022} which use \textit{stochastic diffusion processes}. Such models have replaced the previous generation based on \textit{variational auto encoder} (aka VAE) \citep{Kingma2014}\footnote{nb. notice that \texttt{arxiv} paper have been updated in 2022.} and  \textit{adversarial networks} (aka GAN) \citep{goodfellow2014generative} as in \citep[e.g.]{KarrasALL18,brock2018large}, or \textit{normalizing flows} (hereafter nicknamed NF) such as in \texttt{Glow} \citep{Kingma2018}. 


The ability of generative models have been rapidly adopted in many domains. For instance, in High Energy Physics the reader may be interested by the review \cite{PhysRevD.107.076017}. Concerning galaxy image generation several architectures have been used and some times in conjunction of deblending and deconvoling tasks. For instance \cite{ravanbakhsh2016} use conditional VAE and GAN models, \citep{Schawinski2017,Fussell2019,Hemmati_2022} use GAN models as well and \citep{Arcelin2020} use a VAE model and  \cite{Lanusse2021} propose an hybrid VAE-Normalizing Flow architecture and \cite{smith2021} use a denoising stochastic diffusion model. These generator are intended to go beyond parametrized analytic light profile simulations (e.g. \texttt{GalSim} by \cite{ROWE2015121}) to capture complex structures and interplay between background and single galaxy or blended galaxies signals.  

Despite the impressive image quality of these generative models that are nowadays well diffused in the general public, many questions arise from mathematical and usage point of views. Notably, one may ask for what is really learned by the generative models and what are the statistical properties of the generated samples. In a general manner \cite{Hataya2023} ask if the image generated can corrupt the future datasets. However, the study is devoted  to the large scale statistical models mentioned above that are trained usually with billion-scale data extracted from the Internet, and so in turn can be contaminated by the images shared by many users. Concerning galaxy image generation the datasets are much more modest for the time being (i.e $O(10^5)$) collected from optical surveys (eg. COSMOS HST Advanced Camera for Surveys, \cite{mandelbaum_2019_3242143},  Sloan Digital Sky Survey \citep[SDSS;][]{sdss} Data Release 7 \citep{sdssdr7}) and used by a rather small community compared to social network followers which certainly  limits the datasets corruption. So, besides image corruption, \cite{HACKSTEIN2023100685,janulewicz2024assessing} have investigated different metrics to address the question of fidelity of the generated galaxy images and morphological properties compared to the original dataset.

\cite{kadkhodaie2024generalization} address more mathematically oriented questions and demonstrate the passage between memorization to generalization of  diffusion generative models when the dataset size increases and propose interpretation of what is learned by the network as a sort of \textit{geometry-adaptive harmonic basis} going beyond the commonly used \textit{steerable wavelet basis} first introduced and studied in \cite{Simoncelli1995b,Unser2013} and developped for instance in the \texttt{Kyamato} library \citep{JMLR:v21:19-047}. The authors uses \textit{denoiser} architectures as \texttt{U-Net} \citep{2015arXiv150504597R} and \texttt{BF-CNN} networks \citep{Mohan2020Robust} trained on \texttt{CelebA} \citep{Liu2015} and \texttt{LSUN} bedroom  \citep{Yu2015} reduced datasets of $O(10^5)$ images resized to $80\times 80$ pixels (grayscale). The dataset size and the number of pisels per image make possible to investigate how the \texttt{Glow} normalizing flow based model compare to denoiser based diffusion model concerning the generation of galaxy images both trained with the same dataset issued from DR7 SDSS survey.


In the following sections, first we briefly describe the different kind of generative models and we show the outcome of the conducted numerical experiment before drawing some conclusion.
%
\section{Generative Models}
%
The generative models assume that the observations $\bm{x}\in \mathbb{R}^d$ (in the context of image modelling $d$ is the number of pixels  times the number of channels) are described by a probability distribution $p(\bm{x})$ and aim to give a representation of such distribution.  It is also assumed that the data are i.i.d samples of $p(\bm{x})$.  In the following sections, the intention is more to give some background knowledge rather than describe in great details the different  architecture types and their implementations.
%
\subsection{Variational Auto-Encoder}
%
Using \texttt{VAE} \citep{Kingma2014} means that it is assumed that there exists an underlying unobservable stochastic variable $\bm{z}\in\mathbb{R}^{d_\ell}$  (aka \textit{latent variable}) whose distribution is chosen \textit{a priori} among a parametrized distribution family $\pi_{\bm{\theta}_1}(\bm{z})$ differentiable both with respect to $\bm{z}$ and $\bm{\theta}_1$. In general the latent space has a much lower dimensionality than the data space which imply a information compression ($d_\ell \ll d$). Then, it yields $p(\bm{x}) = p(\bm{x}|\bm{z})\pi_{\bm{\theta}_1}(\bm{z})$. However, the likelihood  $p(\bm{x}|\bm{z})$ is also unknown which leads to the introduction of a parametrized version $p_{\bm{\theta_2}}(\bm{x}|\bm{z})$ with the differentiability properties as $\pi_{\bm{\theta}_1}$. Using the notation $\bm{\theta}=(\bm{\theta_1},\bm{\theta_2})$,  these parameters are adjusted in such a way that  $p_{\bm{\theta}}(x)$ matches the empirical distribution of the $N$ data samples ($\{\bm{x}^{i}\}_{i<N}$). The best solution $\bm{\theta}_{ML}$ is the maximum of $p_{\bm{\theta}}(x)$ but in practice this leads to  intractable problem (either analytical or numerical) as one should evaluate the gradient with respect to $\bm{\theta}$ of the integral 
$\int p_{\bm{\theta}_2}(\bm{x}|\bm{z})\pi_{\bm{\theta}_1}(\bm{z}) dz$ \citep{Kingma2014}. To overcome such problem, one uses first the Bayes relation
\begin{equation}
p_{\bm{\theta}}(x) p(\bm{z}|\bm{x}) = p_{\bm{\theta}_2}(\bm{x}|\bm{z})\pi_{\bm{\theta}_1}(\bm{z})
\end{equation}
and second a parametrized version of the unknown true \textit{a posteriori} distribution as $p_{\bm{\phi}}(\bm{z}|\bm{x})$. If one introduces the Kullback-Leibler divergence $\DKL(p\| q)=\Esp_{\bm{x}\sim p}[\log(p(\bm{x})/q(\bm{x}))]$, then taking the expected value according\footnote{We represent the sampling of variates $\bm{x}$ from a
distribution $p(\bm{x})$ using the notation $\bm{x}\sim p(\bm{x})$.} to $\bm{z}\sim p_{\bm{\phi}}(\bm{z}|\bm{x})$ of the both sides of the following expression
\begin{align}
\log p_{\bm{\theta}}(x) &= \log p_{\bm{\theta}_2}(\bm{x}|\bm{z}) + \log \pi_{\bm{\theta}_1}(\bm{z}) - \log p(\bm{z}|\bm{x}) + \log p_{\bm{\phi}}(\bm{z}|\bm{x}) - \log p_{\bm{\phi}}(\bm{z}|\bm{x})
\end{align}
yields 
\begin{align}
\log p_{\bm{\theta}}(x)  &= \Esp_{z\sim p_{\bm{\phi}}}[\log p_{\bm{\theta}_2}(\bm{x}|\bm{z})] - \DKL(p_{\bm{\phi}}(\bm{z}|\bm{x})\| \pi_{\bm{\theta}_1}(\bm{z})) + \DKL(p_{\bm{\phi}}(\bm{z}|\bm{x})\| p(\bm{z}|\bm{x}))
\geq \mathcal{L}(\bm{x};\{\bm{\theta},\bm{\phi}\})
\label{eq-VAE-ELBO}
\end{align}
where we have emphasized the \textit{evidence lower bound} (aka ELBO)
\begin{equation}
\mathcal{L}(\bm{x};\{\bm{\theta},\bm{\phi}\}) = \Esp_{z\sim p_{\bm{\phi}}}[\log p_{\bm{\theta}_2}(\bm{x}|\bm{z})] - \DKL(p_{\bm{\phi}}(\bm{z}|\bm{x})\| \pi_{\bm{\theta}_1}(\bm{z}))
\end{equation}
and use the positiveness property of the $\DKL$ divergence. Maximizing with respect to 
$\{\bm{\theta},\bm{\phi}\}$ the ELBO is now tractable as it is free of the unknown distributions $p(\bm{x})$ and $p(\bm{z}|\bm{x})$. However, the gradient with respect to $\phi$ may experience large variance, so \cite{Kingma2014} have introduced an auxiliary  variable $\varepsilon$ such that $\bm{z}\sim p_{\bm{\phi}}(\bm{z}|\bm{x})$ is replaced by $\bm{z} = g_{\bm{\phi}}(\bm{\varepsilon},\bm{x})$ with $\bm{\varepsilon} \sim p(\bm{\varepsilon})$ and $g_{\bm{\phi}}(\bm{\varepsilon},\bm{x})$ a differentiable function with respected to $\phi$. 

It is usual to call $p_{\bm{\theta}_2}(\bm{x}|\bm{z})$ the \textit{decoder} part of the whole model while $p_{\bm{\phi}}(\bm{z}|\bm{x})$ is the \textit{encoder} part. The different VAE models differ by the distribution families used. The simplest choice may be commonly-used Gaussian multivariate distributions, and the centred isotropic with unit variance version ($\mathcal{N}(\bm{z},\bm{0},\bm{1}$) for the $\bm{z}$ prior (nb. $\bm{\theta}_1$ are missing is this scenario). Once the optimization is done, the \textit{decoder} is used as a $\bm{x}$ generator by feeding with $\bm{z}$ samples. It turns out that simple VAE tends to generate blurry images and may  suffer from the problem of \textit{posterior collapse} which motivate active ML researches \citep[e.g.][]{engel2018latent,Takida2022} and modified VAE versions to overcome this issue as for instance in \cite{Lanusse2021}. 
%
\subsection{Generative Adversarial Networks}
\label{sec-GAN}
%
The vanilla GAN \citep{goodfellow2014generative} shares with VAE the purpose to optimize a \textit{generator} network ($\bm{G}$) to get $\bm{x}$ samples from a latent variable $\bm{z}$ such that  $\bm{x} = \bm{G}(\bm{z})$ with $\bm{z}\in\mathbb{R}^{d_\ell}$ (in general $d_\ell\ll d$) \textit{prior} $\pi(\bm{z})$ is for instance a Gaussian distribution $\mathcal{N}(\bm{z},\bm{0},\bm{1})$. To do so, a second network ($\bm{D}$) acts as a \textit{discriminator} which aims to state if a sample $\bm{x}$ is from the model distribution or the data distribution. To reach that goal one solves the \texttt{min-max} problem
\begin{equation}
\min_{\bm{G}} \max_{\bm{D}}\left\{ \Esp_{\bm{x}\sim p_{data}(\bm{x})}[\log \bm{D}(\bm{x})] + \Esp_{\bm{z}\sim \pi(\bm{z})}[\log(1-\bm{D}(\bm{G}(\bm{z})))] \right\}
\end{equation} 
where $p_{data}$ is the data generating distribution. Both the  $\bm{G}$ and $\bm{D}$ are parametrized networks. Notice that at parameter of $\bm{G}$ fixed, the optimal \textit{discriminator} $\bm{D}^\ast$ has the following expression
\begin{equation}
\bm{D}^\ast(\bm{x}) = \frac{p_{data}(\bm{x})}{p_{data}(\bm{x}) + p_{\bm{G}}(\bm{x})}
\end{equation}
with $p_{\bm{G}}(\bm{x}) = \pi(\bm{z})|\det J_{\bm{G}} |^{-1}$ using the Jacobian of the \textit{generator}, leading to a reformulation of the \texttt{min-max} problem as followed
\begin{equation}
\min_G \left\{ \DKL{\left( p_{data} \left\| \frac{p_{data} + p_{\bm{G}}}{2}\right. \right) }  + \DKL{\left(p_{\bm{G}} \left\| \frac{p_{data} + p_{\bm{G}}}{2} \right. \right)} \right\}
\end{equation}
leading to the optimal solution $p_{\bm{G}} = p_{data}$ yielding a perfect matching of the data distribution.  But, training GAN model via the above objective function is in general unstable with a pathology called \textit{mode-collapse} in the literature, dealing with the fact that the \textit{discriminator} may overfit the dataset too quickly leading to vanishing gradients \citep{Gulrajani2017}. This training instability has been studied by many authors \citep[see e.g.][for some reviews]{Saxena2021,Jozdani2022}. Architectures like \texttt{BigGAN} \citep{Brock2019} and \texttt{StyleGAN2} \cite{Karras2018ASG,Karras2020} overcome the gradient flow issue by different techniques beeing "state-of-the-art" improvements but at the expense of computational resources. 

\cite{liu2021towards} manage to address the training stability and low  computational resources, in particular low sized datasets, developing a \texttt{light-weight-GAN} structure. A first ingredient is to use the hinge loss introduced by \cite{Lim2017} which aims to minimize the following losses to alternatively train the \textit{discriminator} and the \textit{generator}: 
\begin{align}
\min_{\bm{D}}& \left\{\Esp_{\bm{x}\sim p_{data}}[\max{(0,1-\bm{D}(\bm{x}))}] + \Esp_{\bm{z}\sim \pi(\bm{z})}[\max{(0,1+\bm{D}(\bm{G}(\bm{z}))}]
\right\}\\
\min_{\bm{G}} &\left\{-\Esp_{\bm{z}\sim \pi(\bm{z})}[\bm{D}(\bm{G}(\bm{z}))]
\right\}
\end{align}
Here $\bm{D}$ is a linear discriminator as in the Support Vector Machine context \citep{Vapnik1997}. The second ingredient concerns a strong regularisation to the \textit{discriminator} loss. The \textit{discriminator}  $\bm{D}$  is treated as an \textit{encoder} (e.g. as in a VAE) and is trained together with \textit{decoders} in such a way that $\bm{D}$ is forced to extract image features at different scales ($8\times 8$, $16\times 16$) that the \textit{decoders} can give good reconstruction. Finally, for the \textit{generator} a new skip-layer excitation module (SLE) is used allowing a more robust gradient flow between feature maps extracted at different scales.

%Coccomini2021 utilise Lightweight GAN
% Schawinski2017,Hemmati_2022 GAN vanilla
GAN architectures have been used for galaxy generation in conjunction of deblending and deconvolving tasks: for instance \cite{Schawinski2017,Hemmati_2022} use vanilla-GANs while  \cite{Coccomini2021} the \texttt{light-weight-GAN}.
%
\subsection{Normalizing Flows}
\label{sec-NF}
%
Looking at Equation \ref{eq-VAE-ELBO}, one notices that the optimal solution is reached in case of $\DKL(p_{\bm{\phi}}(\bm{z}|\bm{x})\| p(\bm{z}|\bm{x}))=0$ which is true iif $p_{\bm{\phi}}(\bm{z}|\bm{x}) = p(\bm{z}|\bm{x})$. But this situation cannot be true in general as typical $p_{\bm{\phi}}$ parametrizations involve for instance independent Gaussian distributions. The limited choices of the parametrization is one of the limitation of VAE. So, one motivates the use of normalizing flows (NF) to allow for more flexibility in the distribution family design to hope that the true posterior is member of such family \citep{Tabak2010, Tabak2013a, Rezende2015}. 

A flow $T$ is defined as diffeomorphism (aka bijector) between the data space and a latent space such that
\begin{align}
\bm{x} = T(\bm{z}) & \quad \Leftrightarrow \quad \bm{z} = T^{-1}(\bm{x})
\end{align}
So, if $\bm{z}$ distribution is $\pi(\bm{z})$ (e.g. $\mathcal{N}(\bm{z},\bm{0},\bm{1})$) then the following relation apply
\begin{equation}
p(\bm{x}) =  \pi(\bm{z})|\det J_T(\bm{z})|^{-1} = \pi(T^{-1}(\bm{x}))|\det J_{T^{-1}}(\bm{x})|
\end{equation}
Contrary to VAE and GAN the latent and data space dimensionalities are the same by definition. The flow $T$ is in general a composition of several individual flows $\{T_i\}_{i<n}$ in such a way that
\begin{align}
T=T_1\circ T_2\circ \dots \circ T_n & \quad \Leftrightarrow \quad 
T^{-1}=T_n^{-1}\circ T_{n-1}^{-1}\circ \dots \circ T_1^{-1}
\end{align}
Figure \ref{fig-normflow} represents a schematic views of the \textit{forward} or \textit{generative} direction and the \textit{backward} or \textit{training} direction. 
If one notes $\bm{z}_i = T_i(\bm{z}_{i-1})$ for all $i<n$  (using $\bm{z}_0=\bm{z}$, $\bm{z}_n=\bm{x}$) then
\begin{equation}
\log |\mathrm{det} J_T| = \sum_{i=0}^{n-1} \log |\det J_{T_i}(\bm{z}_{i-1})|
\label{eq-flow-jacob}
\end{equation}
Theorem exists showing that under moderate assumptions flow-based models are able to represent any density distributions \citep{Bogachev2005,huang2019solving}. So, if the flows are well chosen, one can sample $\bm{x}$ with potentially complex (multi-modes) density distribution from a simple spherical centred multivariate Gaussian distribution.


%
\begin{figure}
\resizebox{0.8\columnwidth}{!}{%
%\begin{center}
\begin{tikzpicture}[
node distance = 1cm, 
distrib/.style={rectangle},
]

\node[state] (s0) {$z$};
\node[distrib, above=0.1cm of s0] (img0) {\includegraphics[height=1cm]{fig-nf-z.png}}; 
\node[state, right = of s0] (s1) {$z_1$};
\node[state, right = of s1] (sim1) {$z_{i-1}$};
\node[state, right = of sim1] (si) {$z_{i}$};
\node[distrib, above=0.1cm of si] (imgi) {\includegraphics[height=1cm]{fig-nf-zi.png}}; 
\node[state, right = of si] (snm1) {$z_{n-1}$};
\node[state, right = of snm1] (sn) {$x$};
\node[distrib, above=0.1cm of sn] (imgx) {\includegraphics[height=1cm]{fig-nf-x.png}}; 

\path[-Stealth]
	(s0) edge[bend left] node [above] {$T_1$} (s1)
	(s1) edge[bend left] node [below] {$T_1^{-1}$} (s0)
	(s1) edge[dotted, ultra thick,-] (sim1)
	(sim1) edge[bend left] node [above] {$T_i$}  (si) 
	(si) edge[bend left] node [below] {$T_i^{-1}$}  (sim1) 
	(si) edge[dotted, ultra thick,-](snm1)
	(snm1) edge[bend left] node [above] {$T_n$}  (sn)
	(sn) edge[bend left] node [below] {$T_n^{-1}$}  (snm1); 

\end{tikzpicture}}
\caption{Schematic Normalizing Flow scheme inspired by \cite{weng2018flow}. On the top the \textit{forward} or \textit{generative} direction from a simple $\bm{z}$ distribution to a more complex one for $\bm{x}$. On the bottom the \textit{backward}  or \textit{training} direction from complex to simple distributions.}
\label{fig-normflow}
\end{figure}
%

Let the flow be parametrized by a vector $\bm{\theta}$ leading to a parametrized flow-model $p_{\bm{\theta}}(\bm{x})$. To obtain an optimized flow, we consider the $\DKL$ divergence between  $p_{\bm{\theta}}(\bm{x})$ and the true $p(\bm{x})$ leads to (n.b we consider $\pi(\bm{z})=\mathcal{N}(\bm{z}, \bm{0}, \bm{1})$ without any extra unknown parameters)
\begin{align}
\mathcal{L}(\bm{\theta}) &= \DKL(p(\bm{x})\|p_{\bm{\theta}}(\bm{x}))
= - \Esp_{\bm{x}\sim p(\bm{x})}[-\log p_{\bm{\theta}}(\bm{x})] + \mathrm{const.} \nn \\
&= - \Esp_{\bm{x}\sim p(\bm{x})}[\log \pi(T_{\bm{\theta}}^{-1}(\bm{x}))+ \log |\det J_{T_{\bm{\theta}}^{-1}}(\bm{x})|] + \mathrm{const.} 
\end{align}
In practice, computing the expectation $\Esp_{\bm{x}\sim p(\bm{x})}$ is not possible 
and is replaced by a Monte Carlo technique using the data samples $\{\bm{x}^{(i)}\}_{i<N}$ (dropping the constant)
\begin{equation}
\mathcal{L}(\bm{\theta}) = -\frac{1}{N}\sum_{i=0}^{N-1} \left\{ \log \pi(T_{\bm{\theta}}^{-1}(\bm{x}^{(i)})) + \log |\det J_{T_{\bm{\theta}}^{-1}}(\bm{x}^{(i)})| \right\} 
\end{equation}
The computation of the  determinant of the Jacobian and its gradient may be challenging but more straightforwardly if the Jacobian matrices $\{J_{T_i}^{-1}\}_{i<n}$ (Eq.~\ref{eq-flow-jacob}) are triangular. Notice that we need $T^{-1}$ as well as the Jacobian and the gradients to minimize $\mathcal{L}(\bm{\theta})$ and obtain the  best $\theta$, but we need also $T_\theta$ for the generation of new $\bm{x}$ samples from sampling $z\sim \pi(z)$. Different architectures of normalizing flows as the \textit{autoregressive flows} allowing such program are discussed in the review by \cite{Papamakarios2021}. 

The general schema of \textit{autoregressive flows} is the following: if $\bm{x},\bm{z}\in \mathbb{R}^d$ and noting $\bm{x}=(x_1,x_2,\dots,x_d)$ similarly for $\bm{z}$, then
\begin{equation}
\forall i, \quad x_i = T(z_i;c_i) \quad \mathrm{with} \quad c_i = C_i(z_1,\dots,z_{i-1})=C_i(z_{<i})
\end{equation}
where $T$ a strictly increasing monotonic transformation  and $C_i$ are usually nicknamed \textit{transformer}\footnote{n.b this is not to be confused with the transformer networks that are deep network architecture based on multi-head attention mechanism \citep{Vaswani2017}.} and \textit{conditioner}. The properties of such flows are both the invertibility of the \textit{transformer} and the triangular structure of the Jacobian as
\begin{equation}
J_T = \left[ \frac{\partial x_i}{\partial z_j} \right] =  \begin{cases}
0 & \forall i,j\ s.t.\ i>j \\
\frac{\partial x_i}{\partial z_i}(z_i;h_i) &  \forall i
\end{cases}
\end{equation}
the other elements are not useful, then it yields
\begin{equation}
\log |\det J_T| = \sum_{i=1}^d \log \left| \frac{\partial x_i}{\partial z_i}(z_i;h_i) \right|
\end{equation}
The different versions of transformers and conditioners lead to specific architectures.

Among the transformers, the class of affine transformations are certainly the simplest as they are defined as followed
\begin{equation}
T(z_i;h_i) = e^{\alpha_i} z_i + \beta_i \quad \mathrm{with} \quad c_i = (\alpha_i(z_{<i}),\beta_i(z_{<i}))
\label{eq-affine-coupling}
\end{equation}
(nb. using matrix notation, it yields $T(\bm{z}) = \exp(\bm{\alpha}) \odot \bm{z} + \bm{\beta}$ with  $\odot$ is the Hadamard product).
The invertibility of $T$ is guaranteed by the exponential function. The Jacobian determinant is simply given by the $e^{\alpha_i}$ factor. This kind of affine transformers are used for instance in \citep{DinhKB14,Papamakarios2017a,DinhSB17,Kingma2018}. Notice that \textit{spline} related flows are used by \cite{Crenshaw_2024} (\texttt{PZFlow} code) in the context of galaxy photometric redshift posterior distribution.  

Concerning the conditioners $C_i$, in principle they can be any functions (or models) that input $z_{<i}$ and output $c_i$. But this is the computational cost that drives the schema used in practice. A particular schema is the \textit{coupling layer} implemented \text{Real NVP}\footnote{n.b stands for \textit{real-valued non-volume preserving} transformation.} in \citep{DinhKB14,DinhSB17}. In the context of affine transformation $T$ (Eq.~\ref{eq-affine-coupling}) given $s<d$ (a typical choice is $s=d/2$), the forward pass is defined as 
\begin{equation}
(\mathrm{NVP})\quad  \begin{array}{rcl}
x_{\leq s} &=& z_{\leq s} \\
x_{s+1,\dots,d} &=& z_{s+1,\dots,d} \odot \exp(\bm{\alpha}(z_{\leq s})) + \bm{\beta}(z_{\leq s}) 
\end{array}
\label{eq-NVP_forward}
\end{equation}
The backward pass involving the inverted transform shares the same complexity contrary to other conditioners as masked schema used in \citep{Germain2015} and reads
\begin{equation}
(\mathrm{NVP}^{-1})\quad  \begin{array}{rcl}
z_{\leq s} &=& x_{\leq s} \\
z_{s+1,\dots,d} &=& (x_{s+1,\dots,d}- \bm{\beta}(x_{\leq s}))  \odot \exp(-\bm{\alpha}(x_{\leq s})) \\
&=&(x_{s+1,\dots,d}- \beta(z_{\leq s}))  \odot \exp(-\alpha(z_{\leq s}))
\end{array}
\label{eq-NVP_backward}
\end{equation}
The Jacobian structure is triangular and the diagonal reads
\begin{equation}
\diag(J_T) = (\bm{1}_s,\diag(\exp{\bm{\alpha}(z_{\leq s}))})
\end{equation}
with $\bm{1}_s$ a $s\times s$ identity matrix, leading to $d-s$ \textit{a priori} non-unity terms, and therefore a non unitary transformation implying the \textit{non-volume conservation} property (i.e the NVP notation).

The structure of the transformation (e.g. Eq.~\ref{eq-NVP_forward}) is a single split of the $\bm{z}$ vector coordinates in two parts: the first part $(z_{\leq s})$ is not transformed while the transformation of the second part ${z_{>s}})$ uses a function depending only on the first part. Notice that the conditioner functions do depends only on $(z_{\leq s})$ for both the forward or the backward transformations. This single split schema is illustrated on Figure \ref{fig-nvp}. 
%
\begin{figure}
\centering
\includegraphics[width=0.45\columnwidth]{fig-nvp.pdf}
\includegraphics[width=0.45\columnwidth]{fig-nvp-inverse.pdf}
\caption{Affine coupling layer schema of forward and backward directions (Eqs.~\ref{eq-NVP_forward},\ref{eq-NVP_backward}) with a single split mechanism (figure inspired by \cite{Papamakarios2021}).}
\label{fig-nvp}
\end{figure}
%
This splitting schema implemented in a so-called \textit{coupling layer} makes the flow-based model computationally efficient and using deep neural networks, one can achieve complex $(\bm{\alpha},\bm{\beta})$ functions. Now, designers of complete architectures can compose different \textit{coupling layers} by reversing $\bm{z}$ elements between layers to ensure that all elements are transformed at the end of the set of operations. An generalisation of any permutation ordering of the inputs can be performed using a $1\times 1$ convolution layer. \cite{2016arXiv160508803D} improves the modelling using a multi-scale approach which would be prohibitive to describe here, and \cite{Kingma2018} use all these extensions to build the \texttt{Glow} model whose schema is displayed on Figure \ref{fig-Glow-archi}.
\begin{figure}
    \centering
    \vspace{-0cm}
    \begin{subfigure}[b]{0.45\textwidth}
    \begin{center}
        \centering
        \includegraphics[width=.7\textwidth]{fig-Glow-1.pdf}
        \caption{One step of Glow flow.}
    \end{center}
    \end{subfigure}%
    \vspace{5mm}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=.99\textwidth]{fig-Glow-2.pdf}
        \caption{Multi-scale architecture \citep{2016arXiv160508803D}.}
    \end{subfigure}
    \caption{Figure from \citep{Kingma2018}. The flow step is composed of a scale and bias layer with data dependent initialization (\texttt{actnorm}), a $1\times 1$ invertible convolution layer and an affine coupling layer with $(\bm{\alpha},\bm{\beta})$ issued from a shallow convolutional neural network. Both $\bm{x}$ and $\bm{y}$ are tensors of
shape $[h \times w \times c]$ with spatial dimensions $(h, w)$ and channel dimension $c$ ans $(i, j)$ denote
spatial indices into tensors $\bm{x}$ and $\bm{y}$.  This flow step is embedded in a multi-scale  architecture which has $K$ flow steps and a number of levels $L$ \citep{2016arXiv160508803D}. The squeezing operation reduces a $s \times s \times c$ tensor  into a $s/2\times s/2 \times 4c$ tensor which mix spatial and channel components. At each scale, each channel image is divided into $2\times 2$ patches ($s=2$) and each patch is squeezed. }
    \label{fig-Glow-archi}
\end{figure}
 %
\subsection{Score based diffusion models}
\label{sec-diff}
%
To generate samples from $p(\bm{x})$, we can proceed as follows \citep[e.g][]{Chang2023,LinYang2023}. Starting from a sample $\bm{x}_0$ assumed to be drawn from the distribution $p_0(\bm{x})=p(\bm{x})$, we progressively transform it until the  distribution is as simple as for instance $\mathcal{N}(\bm{z},\bm{0},\bm{1})$.  The idea is then to reverse the process (the \textit{backward} mode) by generating a new sample from a new instance of noise. This transfer mapping looks very similar in concept as normalizing flows. Their latent spaces have the same dimension as the data space, there is no compression as for VAE and GAN architectures, but the main difference consists in the fact that the normalizing flows are deterministic by essence while the \textit{score diffusion} models are stochastic. 

Following the notations used in \cite{kadkhodaie2024generalization}, the transport process used in the \textit{score diffusion} algorithm is the Ornstein-Uhlenbeck equation \citep{Uhlenbeck1930}. The \textit{forward} process follows the following stochastic differential equation:
\begin{equation}
d\bm{x}_t = -\bm{x}_t dt + \sqrt{2}\ dB_t \quad t\in[0,T]
\label{eq-Ornstein-cont}
\end{equation}
where $dB_t$ is a Wiener process (Brownian motion) and $T$ is a finite ending time that would be considered to be large enough. The solution of such forward process reads
\begin{equation}
\bm{x}_t = \bm{x}_0 e^{-t} + \underbrace{(1-e^{-2t})^{1/2}}_{\sigma_t} \bm{z} \qquad \bm{z} \overset{iid}{\sim} \mathcal{N}(\bm{z},\bm{0},\bm{1})
\end{equation}
The associated probability density function is given by the expression 
\begin{equation}
p_t(\bm{x}_t) = \int p_t(\bm{x}_t,\bm{x}_0) d\bm{x}_0 = \int p_t(\bm{x}_t|\bm{x}_0) p_0(\bm{x}_0)dx_0
\label{eq-smoothing-p0-gauss}
\end{equation}
where according to the Fokker-Planck representation of the Ornstein-Uhlenbeck process,  it yields
\begin{equation}
p_t(\bm{x}_t|\bm{x}_0)= \frac{1}{(2\pi \sigma_t^2)^{d/2}}\exp\left\{\displaystyle -\frac{\|\bm{x}_t-\bm{x}_0e^{-t}\|^2}{2\sigma_t^2}\right\}
\end{equation}
So, $p_t(\bm{x}_t)$ is the result of the convolution of the initial distribution $p_0(x_0)$ with a Gaussian with variance $\sigma_t^2$ which gradually tends towards the normal distribution $\mathcal{N}(.,\bm{0},\bm{1})$, producing a progressive blurring that also makes $p_t(\bm{x}_t)$ converging towards the normal distribution. 

Notice that in a VAE architecture,  the transformation from $\bm{x}_0$ to $\bm{z}$ is performed by an \textit{encoder} whose implementation is typically a deep neural networks with trained parameters. This is not the case in the above stochastic process where only noise is added to the original signal (e.g. galaxy image). In the same spirit, the VAE \textit{decoder} or the inverted flows are replaces by stochastic differential equation (damped Langevin)
\begin{equation}
dx_{T-t} = (x_{T-t} + 2\nabla_x \log p_{T-t}(x_{T-t}))dt + \sqrt{2}\ dB_t \quad t\in[0,T]
\label{eq-backward-diffusion}
\end{equation}
where $s_t=\nabla_x \log p_{T-t}(\bm{x}_{T-t})$ is the \textit{score} of the probability density at time $T-t$  of the blurred image $x_{T-t}$ by white noise density of  variance $\sigma^2_{T-t}$. This backward process is the generative direction to get $\bm{x}$ samples from $\mathcal{N}(\bm{z},\bm{0},\bm{1})$ sampling. 

The key problem for the generation is how to obtain the \textit{score}. In this context, one may think of two possible approaches: either one gets a $p_t$ model that can exploit regularity patterns and assumptions about correlations to provide a constrained parametric framework for learning the \textit{score} by \textit{score matching} \citep{hyvarinen2005a}, or one uses a \textit{denoising} neural network as introduced by \cite{Bengio2013} and then \cite{Sohl-Dickstein2015,Ho2020} and recently studied from a mathematical point of view by \citep{kadkhodaie2024generalization}. The former approach has been used by \cite[e.g.][]{Guth2022b,Lempereur2024} using Gibbs energy parametrisation, and the later approach as been used in astrophysics for instance for some applications as the generation of galaxy images \citep{smith2021}, the generation of 21~cm luminosity temperature maps \citep{Zhao2023}, super-resolved large-scale cosmic structure predictions \citep{Schanz2023} and sampling of the high-dimensional Bayesian posterior of the weak lensing mass-mapping problem \citep{Remy2023}.

The relation between a perfect \textit{denoiser} model and the \textit{score} has been pointed by several authors \cite[e.g.][]{tweedie1947functions,herbert1956empirical,miyasawa1961empirical} and more recently by \cite{Raphan2011} for Gaussian white noise $\mathcal{N}(0,\sigma^2)$ independent of the unnoisy signal $x$. Independently of the prior on $\bm{x}$, the following theorem applies: the estimator noted $\tilde{\bm{x}}$ of the signal $\bm{x}$, such that the noisy signal is $\bm{x}_\sigma=\bm{x}+\bm{z}$ with $\bm{z}\sim\mathcal{N}(0,\sigma^2)$, is given by
\begin{equation}
\tilde{\bm{x}}= \bm{x}_\sigma + \sigma^2 \nabla_x \log p(\bm{x}_\sigma)
\label{thm-score-denoising}
\end{equation}
%
So, the use of a \textit{denoiser} network in a generative model is the schematically the following. First, we blindly train the model to recover original images from blurred versions by Gaussian white noise whose variances is not fixed \textit{a priori} and not known by the network and has the same range (e.g $[0,1]$) as the image one. Then, at each step $t$ of the backward stochastic process described above, we use the \textit{denoiser} to obtain the denoised estimate ($\tilde{\bm{x}}$) of the image $\bm{x}_{T-t}$ ($\bm{x}_\sigma$) blurred by a white noise of known variance $\sigma^2=\sigma^2_{T-t}$, so one gets the score $s_t$ to perform one step further in the backward direction.



Denoising diffusion stochastic models have gain in popularity due to the fact that they get as good as results as GAN and have certain advantages. Notably they do not need any adversarial networks, the mathematical ground is clear and robust, the training is also theoretically well defined. But they use an iterative schema for the generation that can be computationally intensive and slower compared to GAN, and the implementation of stochastic processes may require some theoretical expertise even if this argument can be mitigated as expertise is also needed for all kinds of architectures. More discussion about the comparison between the two architectures  can be found in this article \cite{dhariwal2021diffusion}. We do not investigate further the mixing of flow-based and diffusion-based models as \citep[e.g.][]{zhang2021diffusion,gong2021interpreting} as well as the use of diffusion models to improve GAN stability as introduced by \cite{Wang2022}. This can be envisaged in a future study.
%
\section{Experiment}
%
Having briefly outlined different architectures of generative models in the previous section, it turns out that the generation process of a $\bm{x}$ sample may start\footnote{nb. it is the typical implementation choice but one would have chosen uniform distribution as well.} by sampling a centred isotropic Gaussian distribution $\bm{z}\sim \mathcal{N}(\bm{z},\bm{0},\bm{1})$ (i.e. a white noise) of dimension $d_\ell$. In the case of VAE/GAN-based models $d_\ell\ll d$ while $d=d_\ell$ by construction for flow-based and diffusion-based models. But a question arises: if we train two generative models of the same kind, and if we feed them with the same white noise latent array, does the generative process of the two models give the same final image? If not we can ask ourselves about the nature of the learned process and if we are faced to a real probability density $p(\bm{x})$ sample or does the result is a sort of mixture of the original dataset images? 

An answer to the previous question has been given in \cite{kadkhodaie2024generalization} in the context of diffusion-based models and point out the size of the dataset in the passage from \textit{memorization} to \textit{generation} regimes. We propose to use the same methodology by comparing typically two generative models of same kind trained with two different non-overlapping sub-datasets extracted from the same dataset of original images. In this task we cannot be exhaustive and choose only three models and a single dataset in our experiment. We think it is enough to draw some guide lines.
%
\subsection{Dataset}
%
Concerning the training dataset, \cite{kadkhodaie2024generalization}  have used different sources commonly-used in ML as \texttt{CelebA} \citep{Liu2015}, \texttt{LSUN} bedroom \citep{Yu2015} and  \texttt{CelebA HQ} \citep{KarrasALL18} datasets. All dataset images use  gray scale (single channel) ranging in $[0,1]$.  The \texttt{LSUN}  images have been down sampled to $80\times 80$ as well as the \texttt{CelebA} images which also have been further down sampled to $32\times 32$ resolution, while \texttt{CelebA HQ}  have been down sampled to $40\times 40$ resolution.

We are more concerned with galaxy image generation, but as it will be clear in the result section, we need to train two models with up to at least $100,000$ images each, so we have chosen to use the Sloan Digital Sky Survey (SDSS) dataset described in \citep{smith2021}\footnote{See \url{https://github.com/Smith42/astroddpm} to obtain the raw data.} with some modification on the preprocessing. First, all images are cropped about the target coordinate galaxy to a shape of $64\times 64$ pixels, and secondly from the $(z,g,r)$ channels we produce a single  channel in the range $[0,1]$ using \texttt{scipy} \citep{2020SciPy-NMeth} library\footnote{In particular the \texttt{make\_lupton\_rgb} function has been applied with $Q=8$ and $stretch=0.2$ setting}. %Moreover, the gray levels are coded using 5-bit precision in the line of experiment described in \cite{Kingma2018}.
 With such dataset according to \cite{kadkhodaie2024generalization} results, we expect the transition between memorization to generalisation regimes to occur with a number of training images in the range $10^4-10^5$. 

\subsection{Models}
%
Concerning the models, we have chosen three architectures: a GAN, a flow-based and a diffusion-based. 

For GAN model, we have chosen the \texttt{light-weight} architecture described in section \ref{sec-GAN} implemented\footnote{\url{https://github.com/lucidrains/lightweight-gan/}. We have chosen this code has it is more user-friendly compared to the original code given by authors (\url{https://github.com/odegeasslbc/FastGAN-pytorch/tree/main}).} in \texttt{PyTorch} \citep{PyTorch2019}. We have used default architecture settings as the latent space dimension ($d_\ell=256$) as well as the \textit{generator} and \textit{discriminator} (\textit{encoder} and \textit{decoders}) details. The total number of model parameters is about $36$~million.  We do not change as well the default optimizer choice and learning settings, except that first we force any input image to be resized to $128^2$, second we allow only horizontal and vertical flips for input data augmentation. Such model optimization takes about 30~hours for about $100,000$ iterations on a single Nvidia's GPU V100.

Concerning flow-based model, we use the \texttt{Glow} model implemented\footnote{Source code: \url{https://github.com/rosinality/glow-pytorch}} in \texttt{PyTorch}. The architecture has been presented on Figure \ref{fig-Glow-archi} (Sec.~\ref{sec-NF}) and we use $K=32$ and $L=4$ leading to about $61$~million parameters. Concerning the training we use defaults settings and as for the GAN training we allow only horizontal and vertical flips for input data augmentation. For completeness, the image pixel value encoding as been switched from 8-bit to 5-bit following \cite{Kingma2018}. We think that this has a minor impact on our results. We ask for $450,000$ iterations to complete a single model optimization which last approximately 100~hours.

For the diffusion-based model, we have used the same \texttt{U-Net} architecture\footnote{Notice that \cite{2015arXiv150504597R} have also studied a different \textit{denoiser} architecture leading to the same result.} \citep{2015arXiv150504597R} as \textit{denoiser} consisting of a contracting part and an expending part that allows for multi-scale architecture with scale correlations. Details are found in \cite{kadkhodaie2024generalization}\footnote{\url{https://github.com/LabForComputationalVision/memorization_generalization_in_diffusion_models}}. The number of parameters is about $7.6$~million.  The same training procedure used by \cite{kadkhodaie2024generalization} has been applied with 100 epochs, lasting approximately 100~hours.

{\color{red} We refer the reader to the  github repository companion of the present article for the details of the different codes and notebooks used and models trained as well as to get the whole dataset of $250,000$ original galaxy images.}
%
\subsection{Results}
%
\subsubsection{Generated images}
Figure \ref{fig-Original-Glow-UNet-Gan-samples} presents some galaxy images (\textit{validation}) extracted from the original dataset and some samples generated by  the three different models trained with $N=10^5$ images. 
%
\begin{figure}
    \centering
        \includegraphics[width=0.8\textwidth]{fig-model_sample_images.pdf}
    \caption{Some \textit{validation} images from the original dataset (first row from top) and  generates samples from different models trained with $N=10^5$ images:  the \texttt{light-weight} GAN-based model (second row), the \texttt{Glow} flow-based model (third row) and the diffusion-based with \texttt{U-Net} denoiser (bottow row).}
    \label{fig-Original-Glow-UNet-Gan-samples}
\end{figure}
At first look the generated images may be not distinguishable from the original images.
To be a bit more quantitative, Figure \ref{fig-morpho-coeff} presents  some morphological coefficients that have been computed using \texttt{Statmorph}\footnote{\url{https://github.com/vrodgom/statmorph}} \citep{2019MNRAS.483.4140R}: the Concentration, Asymmetry and Smoothness \citep{2000AJ....119.2645B,2003ApJS..147....1C,2004AJ....128..163L}  as well as the Gini and M20 coefficients \citep{2004AJ....128..163L,10.1093/mnras/stv2078}.
\begin{figure}
    \centering
		\includegraphics[width=0.5\textwidth]{fig-CAS-all.pdf}
		\includegraphics[width=0.35\textwidth]{fig-gini-m20-all.pdf}
	\caption{Corner plots\protect\footnotemark of morphological coefficients computed with $10^4$ images, first from the original dataset but \textit{never} used by any models (red, "test"), second from a diffusion-based model generation (blue, "diffusion") and from a flow-based model generation (green, "Glow") (both models used for Figure \ref{fig-Original-Glow-UNet-Gan-samples}). The figure has been inspired by \cite{HACKSTEIN2023100685}.}
	\label{fig-morpho-coeff}
\end{figure}
\footnotetext{Plots from the library \url{https://corner.readthedocs.io} \citep{corner2016}.}
The distributions derived from the \texttt{U-Net} diffusion model are in better agreement with those obtained with the original images than are those derived from the \texttt{Glow} flow model, although this is not dramatic.
Notice that we do not expect high quality images as well as state-of-the-art competitive results, see for instance \citep{ravanbakhsh2016,Fussell2019,Lanusse2021,smith2021,HACKSTEIN2023100685} for better image quality. Our goal is somewhat different: we ask ourselves if with such low resolution and low complexity images, does the models of the same kind learn the same thing with moderate dataset size?
%
\subsubsection{\texttt{U-Net} denoising performances}
Before diving into the generative performances of the models, we present first some results concerning the denoising performances of two \texttt{U-Net} networks trained with different dataset sizes. Figure \ref{fig-UNet-denoising-train} shows how is denoised an original image of the \textit{training set} of both networks. By contrast, to produce the Figure \ref{fig-UNet-denoising-test}, the input original image has never been used to train any networks, this is somewhat a \textit{validation image}. For both figures: the peak signal-to-noise ratio (PSNR) compared to the original image is listed above each image; the top row is composed of the input images (original or noisy) given to a denoiser; for the middle row each image is the output of the \texttt{U-Net} network trained with $N=100$ images, similarly for the bottom row the \texttt{U-Net} network has been trained with $N=100,00$ images. We remind that the networks have been trained with noisy images without any knowledge of the PSNR value (or noise variance). 

Using an image from the \textit{training} dataset, both networks have similar denoising results in terms of the evolution of the output images  PSNR values with respect to the PSNR of the noisy input images. What is the remarkable by contrast is the differences of the  PSNR evolutions when considering the \textit{validation image}: if the \texttt{U-Net} network trained with $N=100,000$ images obtains similar results as for the training image which is the sign of a good generalisation power, this is  not the case for the \texttt{U-Net} network trained with $N=100$ images. For this later  network, the PSNR values of the output images are nearly constant ($\mathrm{PSNR}=27$), even for very low noise level added to the original image. This difference of denoising power between the two networks has an impact on generative process as it starts from pure noise image to progressively output a noiseless image.
\begin{figure}
    \centering
		\includegraphics[width=0.8\textwidth]{fig-UNet_denoising_perf_train.pdf}
	\caption{Performances of two \texttt{U-Net} models using an image of the \textit{training sample}. Top row: (left to right) the original image with progressively a larger noise added. \texttt{PSNR} is defined as $10 \log10$ ratio of squared dynamic range
to mean square error. Middle row: (left to right) image and PSNR obtained after denoising the corresponding image on top row with \texttt{U-Net} trained with a dataset size of $N=100$. Bottom row: same as the middle row but with \texttt{U-Net} trained with  a much larger dataset of $N=100,000$ images.}
	\label{fig-UNet-denoising-train}
\end{figure}

\begin{figure}
    \centering
	\includegraphics[width=0.8\textwidth]{fig-UNet_denoising_perf_test.pdf}
	\caption{Similar to Figure \ref{fig-UNet-denoising-train} but  using an image never used to train any models (i.e. a \textit{validation} image). }
	\label{fig-UNet-denoising-test}
\end{figure}
%
\subsubsection{Evolution of the generative power}
Figure \ref{fig-cosine-diffusion} explores the evolution of the cosine similarity metric \citep{books/aw/TanSK2005} considering either $1,000$ samples generated by the two diffusion-based models trained with independent dataset of same size $N$ and importantly \textit{seeded by the same white noise images} (top), or considering a single model and finding the closest training image (bottom). Despite the fact that the original training images are less diverse as e.g. the \texttt{CelebA} face or the \texttt{LSUN} bedroom images, we experience the following facts: 
\begin{itemize}
\item first: at low sized dataset ($N=100$), the samples generated by the two models look very similar to a training image (cosine similarity close to 1) as shown by bottom left histogram while the samples generated from the same white noise image looks different as the cosine similarity is different from 1 (upper left histogram);
\item second: greater the dataset size is then more and more generated images looks different (from left to right histograms of the lower row) and simultaneously the samples of the two models are more and more similar (from left to right histograms of the upper row). With $N=100,000$, we are close but not yet fully attained to the situation where the two models produce exactly the same output image (in this case a unique bin would be filled at cosine similarity of 1). 
\end{itemize}

\begin{figure}
    \centering
	\includegraphics[width=0.8\textwidth]{fig-diffusion-cosine-AB.pdf}\\
	\includegraphics[width=0.8\textwidth]{fig-diffusion-cosine-ATrain.pdf}
	\caption{Evolution of the cosine similarity metric considering two diffusion-based models trained with two independent datasets of size $N$. On the upper plots are displayed normalized histograms considering $1,000$ generated samples for each of the two models \textit{using as seed the same white noise image}.  On the lower plots, this is the maximum of the metric  using the 1,000 generated samples of one model compared to all images of its corresponding training dataset (in other words this is the cosine of the closest training image from the generated sample). The logarithmic vertical and linear horizontal (cosine value) ranges of each raw are shared among the different histograms. Notice that the horizontal range of the bottom row is restricted to a range closer to the maximum value (i.e. 1) materialized by the red vertical line on each histogram. This figure has been inspired by Figure 2 of \cite{kadkhodaie2024generalization}.}
	\label{fig-cosine-diffusion}
\end{figure}
%
The first fact tell us clearly that the \texttt{U-Net}-based diffusion generative model trained with very few images are simply reproducing the dataset (so-called \textit{memorization} regime), while the second fact is an indication of a rather smooth transition towards a \textit{generation} regime when the dataset size is increasing. These results are in perfect agreement with the ones exposed in \cite{kadkhodaie2024generalization} and originate from the denoising power strength exposed in the previous section. What is surprising is that we still need $10^5$ samples to be confident that the generation regime applies with low complexity and resolution images compared to face or bedroom images.


This last remarks invite ourselves to perform the same experiment with the \texttt{Glow} flow-based model with $N=10^5$ trained images. The result is shown on Figure \ref{fig-cosine-glow}. It is quite remarkable that even if the generated samples seems different from any of the trained images\footnote{Also any of horizontal flipped, vertical flipped or horizontal+vertical flipped versions.} as right histogram does not peak at unity cosine similarity, in the same time even seeded by the same white noise image the two models output images are different as the left histogram does not show a peak at unity cosine similarity too. This is clearly a sign that the two models haven't leaned the same forward transport mapping. 
%
\begin{figure}
    \centering
	\includegraphics[width=0.8\textwidth]{fig-glow-cosine-AB_ATrain.pdf}
	\caption{Histograms similar to those of Figure \ref{fig-cosine-diffusion} for two \texttt{Glow} models using two different datasets of size $N=100,000$ (nb. they are the same as the ones used by the diffusion models). Left: the cosine similarity of $1,000$ samples generated by the two models seeded by the same white noise image. Right: the maximum cosine similarity values of $1,000$ samples generated by one of the models and the training images and their flipped versions (horizontal, vertical and horizontal+vertical) as this data augmentation is used in the training process of \texttt{Glow} models.}
	\label{fig-cosine-glow} 
\end{figure}


%
\begin{figure}
    \centering
	\includegraphics[width=0.8\textwidth]{fig-gan-cosine-AB_ATrain.pdf}
	\caption{Histograms similar to those of Figure \ref{fig-cosine-diffusion} for two \texttt{light-weight} GAN models using two different datasets of size $N=100,000$ (nb. they are the same as the ones used by the diffusion models). Left: the cosine similarity of $1,000$ samples generated by the two models seeded by the same white noise image. Right: the maximum cosine similarity values of $1,000$ samples generated by one of the models and the training images and their flipped versions (horizontal, vertical and horizontal+vertical) as this data augmentation is used in the training process.}
	\label{fig-cosine-gan} 
\end{figure}





\section{Discussion}


\section*{Acknowledgements}
We acknowledges the use of  Nvidia‚Äôs V100 resources from French GENCI‚ÄìIDRIS (Grant 2024-AD010413957R1).

\section*{Codes}

%%%%%%%%%%
\addcontentsline{toc}{section}{References}
% Put your bibiliography file here
%\section{Bibliography}
\bibliographystyle{aa}%{apalike}%{elsarticle-harv}%%{mnras}
\bibliography{references.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\appendix
%\section{}


\end{document}
